{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/aakashkapoor103/anaconda3/Data/src1_1_sample.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "path  = r\"/home/aakashkapoor103/anaconda3/Data/\" \n",
    "\n",
    "names = ['TimeStamp','Host_Name','DiskNumber','Operation_Type','ByteOffset','IOSize','Response_Time','DiskNum']\n",
    "\n",
    "\n",
    "#['TimeStamp','Response','IOType','LUN','ByteOffset','Size']\n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"src1_1_sample.csv\"))\n",
    "print(all_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21267682\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(all_files[0],engine='python',skiprows =4,header=None,na_values=['-1'], index_col=False) \n",
    "names = ['TimeStamp','Host_Name','DiskNumber','Operation_Type','ByteOffset','IOSize','Response_Time','DiskNum']\n",
    "df.columns = names\n",
    "print (len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStamp           int64\n",
      "Host_Name          object\n",
      "DiskNumber        float64\n",
      "Operation_Type     object\n",
      "ByteOffset        float64\n",
      "IOSize            float64\n",
      "Response_Time     float64\n",
      "DiskNum           float64\n",
      "dtype: object\n",
      "21267682\n"
     ]
    }
   ],
   "source": [
    "#Sorting df by TimeStamp\n",
    "\n",
    "df = df.sort_values(by=['TimeStamp'])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(df.dtypes)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "129\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "df['IOSize_log'] = np.log2(df['IOSize'])\n",
    "df['IOSize_log_roundoff']= round(df['IOSize_log'])\n",
    "\n",
    "\n",
    "print(len(Counter(df['IOSize'])))\n",
    "print(len(Counter(df['IOSize_log'])))\n",
    "print(len(Counter(df['IOSize_log_roundoff'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# list_generated = []\n",
    "# delta = 0\n",
    "# list_roundoff = df['IOSize_log_roundoff'].tolist()\n",
    "# list_actual = df['IOSize_log'].tolist()\n",
    "# while(count < len(df)):\n",
    "#     tmp = list_roundoff[count]\n",
    "#     tmp_2 = list_actual[count]\n",
    "#     delta = delta +  abs(tmp - tmp_2)\n",
    "#     count = count + 1\n",
    "# print(delta/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['ByteOffset_Delta'] = df['ByteOffset'] - df['ByteOffset'].shift(-1)\n",
    "df = df.drop(df.index[-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "\n",
    "a = df['ByteOffset_Delta'].unique().tolist()\n",
    "operation_id_map = {}\n",
    "for i,id in enumerate(a): operation_id_map[id] = i \n",
    "df['ByteOffset_Delta_class'] = df['ByteOffset_Delta'].map(lambda x: operation_id_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "x = Counter(df['ByteOffset_Delta_class'])\n",
    "vals = {}\n",
    "vals =  x.most_common(1000)\n",
    "bo_list = []\n",
    "\n",
    "for x in vals:\n",
    "    bo_list.append(x[0])\n",
    "        \n",
    "count = 0\n",
    "label_list = []\n",
    "\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset_Delta_class'].iloc[count]\n",
    "    if x in bo_list:\n",
    "        label_list.append(x)\n",
    "    else:\n",
    "        label_list.append(999999)\n",
    "    count= count + 1\n",
    "    \n",
    "ByteOffset_Delta_class_backup  = df['ByteOffset_Delta_class'] \n",
    "df['ByteOffset_Delta_class']  = label_list\n",
    "print(len(Counter(df['ByteOffset_Delta_class'])))\n",
    "    \n",
    "a = df['ByteOffset_Delta_class'].unique().tolist()\n",
    "bo_map = {}\n",
    "for i,id in enumerate(a): bo_map[id] = i \n",
    "df['ByteOffset_Delta_Class_1001'] = df['ByteOffset_Delta_class'].map(lambda x: bo_map[x])\n",
    "    \n",
    "label_list = df['ByteOffset_Delta_Class_1001'] \n",
    "    \n",
    "df['ByteOffset_Delta_Class_1001']  = label_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Host_Name</th>\n",
       "      <th>DiskNumber</th>\n",
       "      <th>Operation_Type</th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>IOSize</th>\n",
       "      <th>Response_Time</th>\n",
       "      <th>DiskNum</th>\n",
       "      <th>IOSize_log</th>\n",
       "      <th>IOSize_log_roundoff</th>\n",
       "      <th>ByteOffset_Delta</th>\n",
       "      <th>ByteOffset_Delta_class</th>\n",
       "      <th>ByteOffset_Delta_Class_1001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128166372013537059</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3.180110e+09</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-8192.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128166372021660589</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3.180118e+09</td>\n",
       "      <td>28672.0</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.807355</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25980928.0</td>\n",
       "      <td>999999</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128166372021662159</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3.154137e+09</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12288.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128166372021662166</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3.154125e+09</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1348.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-183537664.0</td>\n",
       "      <td>999999</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>128166372023534467</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3.337662e+09</td>\n",
       "      <td>65536.0</td>\n",
       "      <td>4034.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-65536.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TimeStamp Host_Name  DiskNumber Operation_Type    ByteOffset  \\\n",
       "1  128166372013537059      src1         NaN          Write  3.180110e+09   \n",
       "2  128166372021660589      src1         NaN          Write  3.180118e+09   \n",
       "3  128166372021662159      src1         NaN          Write  3.154137e+09   \n",
       "4  128166372021662166      src1         NaN          Write  3.154125e+09   \n",
       "5  128166372023534467      src1         NaN          Write  3.337662e+09   \n",
       "\n",
       "    IOSize  Response_Time  DiskNum  IOSize_log  IOSize_log_roundoff  \\\n",
       "1   8192.0         1507.0      1.0   13.000000                 13.0   \n",
       "2  28672.0         2925.0      1.0   14.807355                 15.0   \n",
       "3   4096.0         1355.0      1.0   12.000000                 12.0   \n",
       "4   4096.0         1348.0      1.0   12.000000                 12.0   \n",
       "5  65536.0         4034.0      1.0   16.000000                 16.0   \n",
       "\n",
       "   ByteOffset_Delta  ByteOffset_Delta_class  ByteOffset_Delta_Class_1001  \n",
       "1           -8192.0                       1                            1  \n",
       "2        25980928.0                  999999                            2  \n",
       "3           12288.0                       3                            3  \n",
       "4      -183537664.0                  999999                            2  \n",
       "5          -65536.0                       5                            4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStamp                        int64\n",
       "Host_Name                       object\n",
       "DiskNumber                     float64\n",
       "Operation_Type                  object\n",
       "ByteOffset                     float64\n",
       "IOSize                         float64\n",
       "Response_Time                  float64\n",
       "DiskNum                        float64\n",
       "IOSize_log                     float64\n",
       "IOSize_log_roundoff            float64\n",
       "ByteOffset_Delta               float64\n",
       "ByteOffset_Delta_class           int64\n",
       "ByteOffset_Delta_Class_1001      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['IOSize_log_roundoff'].unique().tolist()\n",
    "size_map = {}\n",
    "for i,id in enumerate(a): size_map[id] = i \n",
    "df['Size_Class'] = df['IOSize_log_roundoff'].map(lambda x: size_map[x])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop: Index(['TimeStamp', 'Host_Name', 'DiskNumber', 'Operation_Type', 'ByteOffset',\n",
      "       'IOSize', 'Response_Time', 'DiskNum', 'IOSize_log',\n",
      "       'IOSize_log_roundoff', 'ByteOffset_Delta', 'ByteOffset_Delta_class',\n",
      "       'ByteOffset_Delta_Class_1001', 'Size_Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary coloumns\n",
    "\n",
    "print(\"Before drop: {}\".format(df.columns))\n",
    "df.drop(df.columns[[0,1,2,3,6,7,8,10,11]], axis=1,inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteOffset                     float64\n",
       "IOSize                         float64\n",
       "IOSize_log_roundoff            float64\n",
       "ByteOffset_Delta_Class_1001      int64\n",
       "Size_Class                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(Counter(df['Size_Class'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>IOSize</th>\n",
       "      <th>IOSize_log_roundoff</th>\n",
       "      <th>ByteOffset_Delta_Class_1001</th>\n",
       "      <th>Size_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.180110e+09</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.180118e+09</td>\n",
       "      <td>28672.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.154137e+09</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.154125e+09</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.337662e+09</td>\n",
       "      <td>65536.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ByteOffset   IOSize  IOSize_log_roundoff  ByteOffset_Delta_Class_1001  \\\n",
       "1  3.180110e+09   8192.0                 13.0                            1   \n",
       "2  3.180118e+09  28672.0                 15.0                            2   \n",
       "3  3.154137e+09   4096.0                 12.0                            3   \n",
       "4  3.154125e+09   4096.0                 12.0                            2   \n",
       "5  3.337662e+09  65536.0                 16.0                            4   \n",
       "\n",
       "   Size_Class  \n",
       "1           0  \n",
       "2           1  \n",
       "3           2  \n",
       "4           2  \n",
       "5           3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train, validate and test\n",
    "\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "lba_train =df[:training_pt_1]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "lba_test = df[training_pt_1+1:]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "size_train = df[:training_pt_1]['Size_Class'].tolist()\n",
    "size_test = df[training_pt_1+1:]['Size_Class'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lba_train= np.array(lba_train).reshape(-1,1)\n",
    "lba_test= np.array(lba_test).reshape(-1,1)\n",
    "size_train= np.array(size_train).reshape(-1,1)\n",
    "size_test= np.array(size_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset2(dataset, window_size):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - 2 * window_size):\n",
    "        a = dataset[i:(i + window_size), 0]\n",
    "        #print(a)\n",
    "        dataX.append(a)\n",
    "        b = dataset[(i + window_size):(i + 2* window_size), 0]\n",
    "        #print(b)\n",
    "        dataY.append(b)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "lstm_num_timesteps = 32\n",
    "    \n",
    "X_train_lba, y_train_lba = create_dataset2(lba_train, lstm_num_timesteps)\n",
    "X_test_lba, y_test_lba = create_dataset2(lba_test, lstm_num_timesteps)\n",
    "X_train_size, y_train_size = create_dataset2(size_train, lstm_num_timesteps)\n",
    "X_test_size, y_test_size = create_dataset2(size_test, lstm_num_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15950696, 32), (15950696, 32), (5316855, 32), (5316855, 32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lba.shape, y_train_lba.shape, X_test_lba.shape,  y_test_lba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15950696, 32), (15950696, 32), (5316855, 32), (5316855, 32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_size.shape, y_train_size.shape, X_test_size.shape,  y_test_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_num_features = 1\n",
    "lstm_predict_sequences = True\n",
    "lstm_num_predictions = 32\n",
    "\n",
    "\n",
    "# X_train = np.reshape(X_train, (X_train.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "    \n",
    "\n",
    "y_train_lba = np.reshape(y_train_lba, (y_train_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_lba = np.reshape(y_test_lba, (y_test_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_train_size = np.reshape(y_train_size, (y_train_size.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_size = np.reshape(y_test_size, (y_test_size.shape[0], lstm_num_predictions, lstm_num_features))                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle.dump(X_train_lba, open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/src1_X_train_lba.csv\", 'wb'), protocol=4)\n",
    "pickle.dump(X_train_size, open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/src1_X_train_size.csv\", 'wb'), protocol=4)\n",
    "pickle.dump(X_test_lba, open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/src1_X_test_lba.csv\", 'wb'), protocol=4)\n",
    "pickle.dump(X_test_size, open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/src1_X_test_size.csv\", 'wb'), protocol=4)\n",
    "pickle.dump(y_train_lba, open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/src1_y_train_lba.csv\", 'wb'), protocol=4)\n",
    "pickle.dump(y_train_size, open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/src1_y_train_size.csv\", 'wb'), protocol=4)\n",
    "pickle.dump(y_test_lba, open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/src1_y_test_lba.csv\", 'wb'), protocol=4)\n",
    "pickle.dump(y_test_size, open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/src1_y_test_size.csv\", 'wb'), protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle_off = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_X_train_lba.csv\",\"rb\")\n",
    "X_train_lba = pickle.load(pickle_off)\n",
    "\n",
    "pickle_off_1 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_X_train_size.csv\",\"rb\")\n",
    "X_train_size = pickle.load(pickle_off_1)\n",
    "\n",
    "pickle_off_2 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_X_test_lba.csv\",\"rb\")\n",
    "X_test_lba = pickle.load(pickle_off_2)\n",
    "\n",
    "pickle_off_3 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_X_test_size.csv\",\"rb\")\n",
    "X_test_size = pickle.load(pickle_off_3)\n",
    "\n",
    "pickle_off_4 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_y_train_lba.csv\",\"rb\")\n",
    "y_train_lba = pickle.load(pickle_off_4)\n",
    "\n",
    "pickle_off_5 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_y_train_size.csv\",\"rb\")\n",
    "y_train_size = pickle.load(pickle_off_5)\n",
    "\n",
    "pickle_off_6 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_y_test_lba.csv\",\"rb\")\n",
    "y_test_lba = pickle.load(pickle_off_6)\n",
    "\n",
    "pickle_off_7 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_y_test_size.csv\",\"rb\")\n",
    "y_test_size = pickle.load(pickle_off_7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((908210, 32), (908210, 32, 1), (302693, 32), (302693, 32, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lba.shape, y_train_lba.shape, X_test_lba.shape,  y_test_lba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((908210, 32), (908210, 32, 1), (302693, 32), (302693, 32, 1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_size.shape, y_train_size.shape, X_test_size.shape,  y_test_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 341379,\n",
       "         1: 11205893,\n",
       "         2: 3334713,\n",
       "         3: 5287116,\n",
       "         4: 578049,\n",
       "         5: 152495,\n",
       "         6: 119250,\n",
       "         7: 248785})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['Size_Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 1500)     1501500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 32, 1500)     12000       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 3000)     0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32, 1500)     27006000    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 32, 1500)     18006000    lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "offset (TimeDistributed)        (None, 32, 1001)     1502501     lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "iosize (TimeDistributed)        (None, 32, 8)        12008       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 48,040,009\n",
      "Trainable params: 48,040,009\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Two classification outputs\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate , Dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed, Reshape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# no_docs = len(y_train_lba)\n",
    "maxlen= 32\n",
    "\n",
    "\n",
    "\n",
    "# # define two sets of inputs\n",
    "# inputA = Input(shape=(32,))\n",
    "# inputB = Input(shape=(32,))\n",
    "# # inputA = Sequential()\n",
    "# # inputB = Sequential()\n",
    "vocabulary_1 = len(Counter(df['ByteOffset_Delta_Class_1001']))\n",
    "vocabulary_2 = len(Counter(df['Size_Class']))\n",
    "\n",
    "\n",
    "hidden_size = 1500\n",
    "\n",
    "# input=Input(shape=(no_docs,maxlen),dtype='float64')\n",
    "inputA=Input(shape=(maxlen,),dtype='float64')  \n",
    "inputB=Input(shape=(maxlen,),dtype='float64') \n",
    "\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Embedding(input_dim=vocabulary_1,output_dim=hidden_size,input_length=maxlen)(inputA)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# # the second branch opreates on the second input\n",
    "y = Embedding(input_dim=vocabulary_2,output_dim=hidden_size,input_length=maxlen)(inputB)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "# combine the output of the two branches\n",
    "combined = keras.layers.concatenate([x.output, y.output])\n",
    "\n",
    "lstm1 = LSTM(hidden_size,return_sequences=True)(combined)\n",
    "lstm2 = LSTM(hidden_size, return_sequences=True)(lstm1)\n",
    "\n",
    "# create classification output\n",
    "offset = keras.layers.wrappers.TimeDistributed(Dense(units=vocabulary_1, activation='softmax'), name='offset')(lstm2)\n",
    "iosize = keras.layers.wrappers.TimeDistributed(Dense(units=vocabulary_2, activation='softmax'), name='iosize')(lstm2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model =Model([inputA,inputB],[offset,iosize]) # combining all into a Keras model\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'offset': 'sparse_categorical_crossentropy', 'iosize': 'sparse_categorical_crossentropy'},\n",
    "              loss_weights={'offset': 2., 'iosize': 1.5},\n",
    "              metrics={ 'offset': 'categorical_accuracy', 'iosize': 'categorical_accuracy'})\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "data_path = r'/home/aakashkapoor103/anaconda3/Data/Models/'\n",
    "monitor = EarlyStopping(monitor='loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/src1_1_model-trained.hdf5', verbose=1)\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "valid = [X_test_lba,X_test_size],[y_test_lba,y_test_size]\n",
    "\n",
    "model.fit([X_train_lba,X_train_size],[y_train_lba,y_train_size],\n",
    "          verbose=1,epochs=75,callbacks=[monitor,checkpointer],batch_size = 512,validation_data = valid)\n",
    "\n",
    "model.load_weights('best_weights_src1.hdf5') # load weights from best model\n",
    "print('Done ...!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/home/aakashkapoor103/anaconda3/Data/Models/src1_1_model-trained.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c4bebd68e9f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/aakashkapoor103/anaconda3/Data/Models/src1_1_model-trained.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load weights from best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done ...!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/home/aakashkapoor103/anaconda3/Data/Models/src1_1_model-trained.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "model.load_weights('/home/aakashkapoor103/anaconda3/Data/Models/src1_1_model-trained.hdf5') # load weights from best model\n",
    "print('Done ...!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/aakashkapoor103/anaconda3/Data/Models/src1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model('/home/aakashkapoor103/anaconda3/Data/Models/src1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302693/302693 [==============================] - 151s 499us/step\n",
      "151.01667070388794\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pred1,pred2 = new_model.predict([X_test_lba,X_test_size],verbose =1 )\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498.91035043389815"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time/len(X_test_lba)*1000*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train, validate and test\n",
    "\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "lba_train =df[:training_pt_1]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "lba_test = df[training_pt_1+1:]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "size_train = df[:training_pt_1]['Size_class'].tolist()\n",
    "size_test = df[training_pt_1+1:]['Size_class'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "lba_test_final = lba_test[-(len(pred1)):]\n",
    "lba_size_final = size_test[-(len(pred1)):]\n",
    "\n",
    "max_lba_accuracy = 0\n",
    "max_lba_accuracy_pos = 0\n",
    "\n",
    "max_size_accuracy = 100000\n",
    "max_size_accuracy_pos = 0\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pred_1 = pred1[:,0,:]\n",
    "pred_2 = pred2[:,0,:]\n",
    "pred_1 = np.argmax(pred_1, axis=1)\n",
    "pred_2 = np.argmax(pred_2, axis=1)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "lba_accuracy = accuracy_score(lba_test_final, pred_1)\n",
    "\n",
    "\n",
    "size_accuracy = accuracy_score(lba_size_final, pred_2)\n",
    "\n",
    "print(\"IO Size Accuracy\", str(size_accuracy))\n",
    "print(\"Best LBA Delta Accuracy\", str(lba_accuracy))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names= ['IOSize_pred','IOSize_actual','LBA_pred','LBA_actual']\n",
    "import pandas as pd\n",
    "df_res = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['IOSize_actual'] = lba_size_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['IOSize_pred'] = pred_2\n",
    "df_res['IOSize_actual'] = lba_size_final\n",
    "df_res['LBA_pred'] = pred_1\n",
    "df_res['LBA_actual'] = lba_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['IOSize_log_roundoff'] = df['IOSize_log_roundoff'][-(len(pred1)):].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "lba_hit = 0\n",
    "block_hit = 0\n",
    "total_block = 0\n",
    "\n",
    "while(count<len(df_res)):\n",
    "    if(df_res['LBA_pred'].iloc[count] == df_res['LBA_actual'].iloc[count]):\n",
    "        lba_hit = lba_hit +1\n",
    "        if(df_res['IOSize_actual'].iloc[count] == df_res['IOSize_actual'].iloc[count]):\n",
    "            block_hit = block_hit + (df_res['IOSize_actual'].iloc[count])\n",
    "    total_block = total_block + df_res['IOSize_actual'].iloc[count]\n",
    "    count = count + 1\n",
    "    \n",
    "print(block_hit/total_block)\n",
    "print(lba_hit/count)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
