{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install Pillow\n",
    "\n",
    "# Create 2 separate input embed layers\n",
    "# Connect to common LSTM-LSTM\n",
    "# Branch out and predict IOSIze and Offset differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/SYSTOR/part27/2016030819-LUN2.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path  = r\"/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/SYSTOR/part27/\" \n",
    "\n",
    "names = ['TimeStamp','Response','IOType','LUN','ByteOffset','Size']\n",
    "\n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"2016030819-LUN2.csv\"))\n",
    "print(all_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2902328\n",
      "1000\n",
      "Percentage Coverage\n",
      "68.94633168488595\n",
      "1000\n",
      "Percentage Coverage Offset\n",
      "69.80925994899955\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(all_files[0],engine='python',skiprows =1,header=None,na_values=['-1'], index_col=False) \n",
    "names = ['TimeStamp','Response','IOType','LUN','ByteOffset','Size']   \n",
    "df.columns = names\n",
    "print (len(df))\n",
    "df['ByteOffset_Delta'] = df['ByteOffset'] - df['ByteOffset'].shift(-1)\n",
    "df = df.drop(df.index[-1])\n",
    "\n",
    "total_classes = len(Counter(df['ByteOffset_Delta']))\n",
    "x = Counter(df['ByteOffset_Delta'])\n",
    "vals = {}\n",
    "vals =  x.most_common(1000)\n",
    "bo_list = []\n",
    "coverage = 0\n",
    "for key,value in vals:\n",
    "    bo_list.append(key)\n",
    "    coverage = coverage + value\n",
    "print(len(bo_list))\n",
    "print(\"Percentage Coverage\")\n",
    "print((coverage/len(df))*100) \n",
    "total_classes = len(Counter(df['ByteOffset']))\n",
    "x = Counter(df['ByteOffset'])\n",
    "vals = {}\n",
    "vals =  x.most_common(1000)\n",
    "bo_list = []\n",
    "coverage = 0\n",
    "for key,value in vals:\n",
    "    bo_list.append(key)\n",
    "    coverage = coverage + value\n",
    "    \n",
    "print(len(bo_list))\n",
    "print(\"Percentage Coverage Offset\")\n",
    "print((coverage/len(df))*100)\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "len(Counter(df['Size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "\n",
    "a = df['ByteOffset_Delta'].unique().tolist()\n",
    "operation_id_map = {}\n",
    "for i,id in enumerate(a): operation_id_map[id] = i \n",
    "df['ByteOffset_Delta_class'] = df['ByteOffset_Delta'].map(lambda x: operation_id_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['Size'].unique().tolist()\n",
    "size_id_map = {}\n",
    "for i,id in enumerate(a): size_id_map[id] = i \n",
    "df['Size_class'] = df['Size'].map(lambda x: operation_id_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStamp                 float64\n",
      "Response                  float64\n",
      "IOType                     object\n",
      "LUN                         int64\n",
      "ByteOffset                  int64\n",
      "Size                        int64\n",
      "ByteOffset_Delta          float64\n",
      "ByteOffset_Delta_class      int64\n",
      "Size_class                  int64\n",
      "dtype: object\n",
      "2902327\n"
     ]
    }
   ],
   "source": [
    "#Sorting df by TimeStamp\n",
    "\n",
    "df = df.sort_values(by=['TimeStamp'])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(df.dtypes)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "x = Counter(df['ByteOffset_Delta_class'])\n",
    "vals = {}\n",
    "vals =  x.most_common(1000)\n",
    "bo_list = []\n",
    "\n",
    "for x in vals:\n",
    "    bo_list.append(x[0])\n",
    "        \n",
    "count = 0\n",
    "label_list = []\n",
    "\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset_Delta_class'].iloc[count]\n",
    "    if x in bo_list:\n",
    "        label_list.append(x)\n",
    "    else:\n",
    "        label_list.append(999999)\n",
    "    count= count + 1\n",
    "    \n",
    "ByteOffset_Delta_class_backup  = df['ByteOffset_Delta_class'] \n",
    "df['ByteOffset_Delta_class']  = label_list\n",
    "print(len(Counter(df['ByteOffset_Delta_class'])))\n",
    "    \n",
    "a = df['ByteOffset_Delta_class'].unique().tolist()\n",
    "bo_map = {}\n",
    "for i,id in enumerate(a): bo_map[id] = i \n",
    "df['ByteOffset_Delta_Class_1001'] = df['ByteOffset_Delta_class'].map(lambda x: bo_map[x])\n",
    "    \n",
    "label_list = df['ByteOffset_Delta_Class_1001'] \n",
    "    \n",
    "df['ByteOffset_Delta_Class_1001']  = label_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amin, amax = df['Size'].min(), df['Size'].max()\n",
    "# a = df['Size'].tolist()\n",
    "# for i, val in enumerate(a):\n",
    "#     a[i] = (val-amin) / (amax-amin)\n",
    "    \n",
    "# df['Size'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Response</th>\n",
       "      <th>IOType</th>\n",
       "      <th>LUN</th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>Size</th>\n",
       "      <th>ByteOffset_Delta</th>\n",
       "      <th>ByteOffset_Delta_class</th>\n",
       "      <th>Size_class</th>\n",
       "      <th>ByteOffset_Delta_Class_1001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.457431e+09</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>R</td>\n",
       "      <td>2</td>\n",
       "      <td>5119596273664</td>\n",
       "      <td>4096</td>\n",
       "      <td>1.342012e+12</td>\n",
       "      <td>999999</td>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.457431e+09</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>R</td>\n",
       "      <td>2</td>\n",
       "      <td>5119596236800</td>\n",
       "      <td>4096</td>\n",
       "      <td>3.857761e+12</td>\n",
       "      <td>999999</td>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.457431e+09</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>W</td>\n",
       "      <td>2</td>\n",
       "      <td>3777583883264</td>\n",
       "      <td>12288</td>\n",
       "      <td>-3.002076e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.457431e+09</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>W</td>\n",
       "      <td>2</td>\n",
       "      <td>3777884090880</td>\n",
       "      <td>8192</td>\n",
       "      <td>-1.444479e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>570</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.457431e+09</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>W</td>\n",
       "      <td>2</td>\n",
       "      <td>3779328570368</td>\n",
       "      <td>8192</td>\n",
       "      <td>1.596548e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>570</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TimeStamp  Response IOType  LUN     ByteOffset   Size  ByteOffset_Delta  \\\n",
       "0  1.457431e+09  0.005598      R    2  5119596273664   4096      1.342012e+12   \n",
       "1  1.457431e+09  0.004451      R    2  5119596236800   4096      3.857761e+12   \n",
       "2  1.457431e+09  0.000162      W    2  3777583883264  12288     -3.002076e+08   \n",
       "3  1.457431e+09  0.000139      W    2  3777884090880   8192     -1.444479e+09   \n",
       "4  1.457431e+09  0.000148      W    2  3779328570368   8192      1.596548e+09   \n",
       "\n",
       "   ByteOffset_Delta_class  Size_class  ByteOffset_Delta_Class_1001  \n",
       "0                  999999         919                            0  \n",
       "1                  999999         919                            0  \n",
       "2                       1         175                            1  \n",
       "3                       2         570                            2  \n",
       "4                       3         570                            3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStamp                      float64\n",
       "Response                       float64\n",
       "IOType                          object\n",
       "LUN                              int64\n",
       "ByteOffset                       int64\n",
       "Size                             int64\n",
       "ByteOffset_Delta               float64\n",
       "ByteOffset_Delta_class           int64\n",
       "Size_class                       int64\n",
       "ByteOffset_Delta_Class_1001      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop: Index(['TimeStamp', 'Response', 'IOType', 'LUN', 'ByteOffset', 'Size',\n",
      "       'ByteOffset_Delta', 'ByteOffset_Delta_class', 'Size_class',\n",
      "       'ByteOffset_Delta_Class_1001'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary coloumns\n",
    "\n",
    "print(\"Before drop: {}\".format(df.columns))\n",
    "df.drop(df.columns[[0,1,2,3,4,5,6,7]], axis=1,inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Size_class                     int64\n",
       "ByteOffset_Delta_Class_1001    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2176745\n",
      "725581\n"
     ]
    }
   ],
   "source": [
    "# Split to train, validate and test\n",
    "\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "\n",
    "data_train = df[:training_pt_1]\n",
    "data_test = df[training_pt_1+1:]\n",
    "\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train =df[:training_pt_1]['ByteOffset_Delta_Class_1001'].values.tolist()\n",
    "data_test = df[training_pt_1+1:]['ByteOffset_Delta_Class_1001'].values.tolist()\n",
    "size_train = df[:training_pt_1]['Size_class'].values.tolist()\n",
    "size_test = df[training_pt_1+1:]['Size_class'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = r'/soe/cchakrab/test_output/output_csv/SYSTOR/p3/'\n",
    "\n",
    "path_train  = r\"/soe/cchakrab/test_output/output_csv/SYSTOR/p3/delta_train.txt\"\n",
    "\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in data_train:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n",
    "path_test  = r\"/soe/cchakrab/test_output/output_csv/SYSTOR/p3/delta_test.txt\"\n",
    "with open(path_test, 'w') as f:\n",
    "    for item in data_test:\n",
    "        f.write(\"%s \" % item)\n",
    "\n",
    "\n",
    "path_train  = r\"/soe/cchakrab/test_output/output_csv/SYSTOR/p3/size_train.txt\"\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in size_train:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n",
    "path_test  = r\"/soe/cchakrab/test_output/output_csv/SYSTOR/p3/size_test.txt\"\n",
    "with open(path_test, 'w') as f:\n",
    "    for item in size_test:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "min_size = df['Size'].min()\n",
    "\n",
    "\n",
    "def get_data_generator(df, indices, for_training, batch_size=16):\n",
    "    lba_delta, io_size = [], []\n",
    "    while True:\n",
    "        for i in indices:\n",
    "            r = df.iloc[i]\n",
    "            delta, size_io = r['ByteOffset_Delta_Class_1001'], r['Size']\n",
    "            lba_delta.append(delta)\n",
    "            io_size.append(size_io / min_size)\n",
    "            \n",
    "            if len(lba_delta) >= batch_size:\n",
    "                yield np.array(lba_delta),  np.array(io_size)\n",
    "                lba_delta, io_size = [], []\n",
    "        if not for_training:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.7\n",
    "count = 0\n",
    "p = []\n",
    "while(count <len(df)):\n",
    "    p.append(count)\n",
    "    count = count+1\n",
    "\n",
    "\n",
    "train_up_to = int(len(df) * TRAIN_TEST_SPLIT)\n",
    "train_idx = p[:train_up_to]\n",
    "test_idx = p[train_up_to:]\n",
    "\n",
    "# split train_idx further into training and validation set\n",
    "train_up_to = int(train_up_to * 0.7)\n",
    "train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "\n",
    "num_steps = 32\n",
    "hidden_size = 1500 \n",
    "use_dropout=True \n",
    "vocabulary_1 = 1001\n",
    "vocabulary_2 = 259\n",
    "\n",
    "\n",
    "# define two sets of inputs\n",
    "inputA = Input(shape=(32,))\n",
    "inputB = Input(shape=(32,))\n",
    "# the first branch operates on the first input\n",
    "x = Embedding(vocabulary_1, hidden_size, input_length=32)(inputA)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "# the second branch opreates on the second input\n",
    "y = Embedding(vocabulary_2, hidden_size, input_length=32)(inputB)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output])\n",
    "# # apply a FC layer and then a regression prediction on the\n",
    "# # combined outputs\n",
    "# z = Dense(2, activation=\"relu\")(combined)\n",
    "# z = Dense(1, activation=\"linear\")(z)\n",
    "# # our model will accept the inputs of the two branches and\n",
    "# # then output a single value\n",
    "# model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "\n",
    "lstm1 = LSTM(hidden_size, return_sequences=True)(combined)\n",
    "lstm2 = LSTM(hidden_size, return_sequences=True)(lstm1)\n",
    "\n",
    "X= Dense(units=128, activation='relu')(lstm2)\n",
    "iosize = Dense(units=1, activation='sigmoid', name='iosize')(X)\n",
    "\n",
    "# create classification output\n",
    "offset = Dense(units=1001, activation='relu', name='offset')(lstm2)\n",
    "\n",
    "\n",
    "model = Model(inputs=[x.input, y.input], outputs=[iosize, offset])\n",
    "model.compile(optimizer=Adam(), \n",
    "              loss={'iosize': 'mse', 'offset': 'categorical_crossentropy'},\n",
    "              loss_weights={'iosize': 1., 'offset': 1},\n",
    "              metrics={'iosize': 'mae', 'offset': 'accuracy'})\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "train_gen = get_data_generator(df, train_idx, for_training=True, batch_size=batch_size)\n",
    "valid_gen = get_data_generator(df, valid_idx, for_training=True, batch_size=valid_batch_size)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
    "]\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=len(train_idx)//batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose =1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=len(valid_idx)//valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=test_data_generator.generate(),\n",
    "                        validation_steps=len(train_data)//(batch_size*num_steps), callbacks=[checkpointer,monitor])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_tf_gpu] *",
   "language": "python",
   "name": "conda-env-py3_tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
