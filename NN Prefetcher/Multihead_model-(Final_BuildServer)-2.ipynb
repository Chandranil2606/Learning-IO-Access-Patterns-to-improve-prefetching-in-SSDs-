{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/Buildserver/data-output-buildserver-2_total.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path  = r\"/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/Buildserver/\" \n",
    "\n",
    "names = ['Operation','TimeStamp','Process_Name','ThreadID','IrpPtr','ByteOffset','IOSize','ThreadID','ElapsedTime','DiskNum','IrpFlags','DiskSvcTime','I/O Pri','VolSnap','FileObject','FileName','IO_Pri']\n",
    "\n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"data-output-buildserver-2_total.csv\"))\n",
    "print(all_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600438\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(all_files[0],engine='python',skiprows =1,header=None,na_values=['-1'], index_col=False) \n",
    "names = ['Operation','TimeStamp','Process_Name','ThreadID','IrpPtr','ByteOffset','IOSize','ThreadID','ElapsedTime','DiskNum','IrpFlags','DiskSvcTime','I/O Pri','VolSnap','FileObject','FileName','IO_Pri']\n",
    "df.columns = names\n",
    "print (len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation        object\n",
      "TimeStamp        object\n",
      "Process_Name     object\n",
      "ThreadID         object\n",
      "IrpPtr           object\n",
      "ByteOffset       object\n",
      "IOSize           object\n",
      "ThreadID         object\n",
      "ElapsedTime      object\n",
      "DiskNum          object\n",
      "IrpFlags         object\n",
      "DiskSvcTime      object\n",
      "I/O Pri         float64\n",
      "VolSnap          object\n",
      "FileObject       object\n",
      "FileName         object\n",
      "IO_Pri           object\n",
      "dtype: object\n",
      "1600438\n"
     ]
    }
   ],
   "source": [
    "#Sorting df by TimeStamp\n",
    "\n",
    "df = df.sort_values(by=['TimeStamp'])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(df.dtypes)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600430\n",
      "1600430\n"
     ]
    }
   ],
   "source": [
    "addresses = df['ByteOffset'].tolist()\n",
    "size = df['IOSize'].tolist()\n",
    "addresses_dec = []\n",
    "size_dec = []\n",
    "count = 0\n",
    "\n",
    "while (count < len(addresses)):\n",
    "    if \"Offset\" in addresses[count]:\n",
    "        count = count +1\n",
    "        continue\n",
    "    dec = addresses[count]\n",
    "    dec_size = addresses[count]\n",
    "    tmp = int(dec, 16)\n",
    "    tmp_size = int(dec_size,16)\n",
    "    addresses_dec.append(tmp)\n",
    "    size_dec.append(tmp_size)\n",
    "    count = count +1\n",
    "    \n",
    "print(len(addresses_dec))\n",
    "print(len(size_dec))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630\n",
      "1329410\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(Counter(df['IOSize'])))\n",
    "print(len(Counter(df['ByteOffset'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.DataFrame(columns = ['ByteOffset', 'IOSize']) \n",
    "\n",
    "df_a['ByteOffset'] = addresses_dec\n",
    "df_a['Size'] = size_dec\n",
    "\n",
    "df = df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>IOSize</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>251824893952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251824893952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319350947840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319350947840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81801076736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81801076736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186105143296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>186105143296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240879329280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240879329280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ByteOffset IOSize          Size\n",
       "0  251824893952    NaN  251824893952\n",
       "1  319350947840    NaN  319350947840\n",
       "2   81801076736    NaN   81801076736\n",
       "3  186105143296    NaN  186105143296\n",
       "4  240879329280    NaN  240879329280"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ByteOffset_Delta'] = df['ByteOffset'] - df['ByteOffset'].shift(-1)\n",
    "df = df.drop(df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "\n",
    "a = df['ByteOffset_Delta'].unique().tolist()\n",
    "operation_id_map = {}\n",
    "for i,id in enumerate(a): operation_id_map[id] = i \n",
    "df['ByteOffset_Delta_class'] = df['ByteOffset_Delta'].map(lambda x: operation_id_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = df['IOSize'].unique().tolist()\n",
    "# size_id_map = {}\n",
    "# for i,id in enumerate(a): size_id_map[id] = i \n",
    "# df['Size_class'] = df['IOSize'].map(lambda x: size_id_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "x = Counter(df['ByteOffset_Delta_class'])\n",
    "vals = {}\n",
    "vals =  x.most_common(1000)\n",
    "bo_list = []\n",
    "\n",
    "for x in vals:\n",
    "    bo_list.append(x[0])\n",
    "        \n",
    "count = 0\n",
    "label_list = []\n",
    "\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset_Delta_class'].iloc[count]\n",
    "    if x in bo_list:\n",
    "        label_list.append(x)\n",
    "    else:\n",
    "        label_list.append(999999)\n",
    "    count= count + 1\n",
    "    \n",
    "ByteOffset_Delta_class_backup  = df['ByteOffset_Delta_class'] \n",
    "df['ByteOffset_Delta_class']  = label_list\n",
    "print(len(Counter(df['ByteOffset_Delta_class'])))\n",
    "    \n",
    "a = df['ByteOffset_Delta_class'].unique().tolist()\n",
    "bo_map = {}\n",
    "for i,id in enumerate(a): bo_map[id] = i \n",
    "df['ByteOffset_Delta_Class_1001'] = df['ByteOffset_Delta_class'].map(lambda x: bo_map[x])\n",
    "    \n",
    "label_list = df['ByteOffset_Delta_Class_1001'] \n",
    "    \n",
    "df['ByteOffset_Delta_Class_1001']  = label_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(df['ByteOffset_Delta_Class_1001']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>IOSize</th>\n",
       "      <th>Size</th>\n",
       "      <th>ByteOffset_Delta</th>\n",
       "      <th>ByteOffset_Delta_class</th>\n",
       "      <th>ByteOffset_Delta_Class_1001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>251824893952</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251824893952</td>\n",
       "      <td>-6.752605e+10</td>\n",
       "      <td>999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319350947840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319350947840</td>\n",
       "      <td>2.375499e+11</td>\n",
       "      <td>999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81801076736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81801076736</td>\n",
       "      <td>-1.043041e+11</td>\n",
       "      <td>999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186105143296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>186105143296</td>\n",
       "      <td>-5.477419e+10</td>\n",
       "      <td>999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240879329280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240879329280</td>\n",
       "      <td>1.861192e+11</td>\n",
       "      <td>999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ByteOffset IOSize          Size  ByteOffset_Delta  \\\n",
       "0  251824893952    NaN  251824893952     -6.752605e+10   \n",
       "1  319350947840    NaN  319350947840      2.375499e+11   \n",
       "2   81801076736    NaN   81801076736     -1.043041e+11   \n",
       "3  186105143296    NaN  186105143296     -5.477419e+10   \n",
       "4  240879329280    NaN  240879329280      1.861192e+11   \n",
       "\n",
       "   ByteOffset_Delta_class  ByteOffset_Delta_Class_1001  \n",
       "0                  999999                            0  \n",
       "1                  999999                            0  \n",
       "2                  999999                            0  \n",
       "3                  999999                            0  \n",
       "4                  999999                            0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329409\n",
      "1329409\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df['IOSize_log'] = np.log2(df['Size'])\n",
    "\n",
    "df['IOSize_log_roundoff']= round(df['IOSize_log'])\n",
    "print(len(Counter(df['Size'])))\n",
    "print(len(Counter(df['IOSize_log'])))\n",
    "print(len(Counter(df['IOSize_log_roundoff'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['IOSize_log_roundoff'].unique().tolist()\n",
    "size_id_map = {}\n",
    "for i,id in enumerate(a): size_id_map[id] = i \n",
    "df['Size_class'] = df['IOSize_log_roundoff'].map(lambda x: size_id_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(df['ByteOffset_Delta_Class_1001']))\n",
    "len(Counter(df['Size_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train, validate and test\n",
    "\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "lba_train =df[:training_pt_1]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "lba_test = df[training_pt_1+1:]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "size_train = df[:training_pt_1]['Size_class'].tolist()\n",
    "size_test = df[training_pt_1+1:]['Size_class'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lba_train= np.array(lba_train).reshape(-1,1)\n",
    "lba_test= np.array(lba_test).reshape(-1,1)\n",
    "size_train= np.array(size_train).reshape(-1,1)\n",
    "size_test= np.array(size_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset2(dataset, window_size):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - 2 * window_size):\n",
    "        a = dataset[i:(i + window_size), 0]\n",
    "        #print(a)\n",
    "        dataX.append(a)\n",
    "        b = dataset[(i + window_size):(i + 2* window_size), 0]\n",
    "        #print(b)\n",
    "        dataY.append(b)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "lstm_num_timesteps = 32\n",
    "    \n",
    "X_train_lba, y_train_lba = create_dataset2(lba_train, lstm_num_timesteps)\n",
    "X_test_lba, y_test_lba = create_dataset2(lba_test, lstm_num_timesteps)\n",
    "X_train_size, y_train_size = create_dataset2(size_train, lstm_num_timesteps)\n",
    "X_test_size, y_test_size = create_dataset2(size_test, lstm_num_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200257, 32), (1200257, 32), (400043, 32), (400043, 32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lba.shape, y_train_lba.shape, X_test_lba.shape,  y_test_lba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200257, 32), (1200257, 32), (400043, 32), (400043, 32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_size.shape, y_train_size.shape, X_test_size.shape,  y_test_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "file__1 = open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver_2_X_train_lba.csv', 'wb')\n",
    "pkl.dump(X_train_lba, file__1)\n",
    "file__1.close()\n",
    "\n",
    "file__1 = open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver_2_X_train_size.csv', 'wb')\n",
    "pkl.dump(X_train_size, file__1)\n",
    "file__1.close()\n",
    "\n",
    "\n",
    "file__1 = open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver_2_X_test_lba.csv', 'wb')\n",
    "pkl.dump(X_test_lba, file__1)\n",
    "file__1.close()\n",
    "\n",
    "file__1 = open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver_2_X_test_size.csv', 'wb')\n",
    "pkl.dump(X_test_size, file__1)\n",
    "file__1.close()\n",
    "\n",
    "\n",
    "file__1 = open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver_2_y_train_lba.csv', 'wb')\n",
    "pkl.dump(y_train_lba, file__1)\n",
    "file__1.close()\n",
    "\n",
    "file__1 = open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver_2_y_train_size.csv', 'wb')\n",
    "pkl.dump(y_train_size, file__1)\n",
    "file__1.close()\n",
    "\n",
    "file__1 = open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver_2_y_test_lba.csv', 'wb')\n",
    "pkl.dump(y_test_lba, file__1)\n",
    "file__1.close()\n",
    "\n",
    "\n",
    "file__1 = open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver_2_y_test_size.csv', 'wb')\n",
    "pkl.dump(y_test_size, file__1)\n",
    "file__1.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver-2_X_train_lba.csv','rb') as f: X_train_lba = pickle.load(f)\n",
    "# with open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver-2_X_train_size.csv','rb') as f: X_train_size = pickle.load(f)\n",
    "# with open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver-2_X_train_lba.csv','rb') as f: X_test_lba = pickle.load(f)\n",
    "# with open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver-2_X_train_lba.csv','rb') as f: X_test_size = pickle.load(f)\n",
    "\n",
    "                \n",
    "# with open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver-2_y_train_lba.csv','rb') as f: y_train_lba = pickle.load(f)\n",
    "# with open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver-2_y_train_size.csv','rb') as f: y_train_size = pickle.load(f)\n",
    "# with open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver-2_y_test_lba.csv','rb') as f: y_test_lba = pickle.load(f)\n",
    "# with open('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/numpy_array_save/buildserver-2_y_test_size.csv','rb') as f: y_test_size = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200257, 32), (1200257, 32), (400043, 32), (400043, 32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_lba.shape, y_train_lba.shape, X_test_lba.shape,  y_test_lba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200257, 32), (1200257, 32), (400043, 32), (400043, 32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_size.shape, y_train_size.shape, X_test_size.shape,  y_test_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_num_features = 1\n",
    "lstm_predict_sequences = True\n",
    "lstm_num_predictions = 32\n",
    "\n",
    "\n",
    "# X_train = np.reshape(X_train, (X_train.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "    \n",
    "\n",
    "y_train_lba = np.reshape(y_train_lba, (y_train_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_lba = np.reshape(y_test_lba, (y_test_lba.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_train_size = np.reshape(y_train_size, (y_train_size.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "y_test_size = np.reshape(y_test_size, (y_test_size.shape[0], lstm_num_predictions, lstm_num_features))                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 500)      500500      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 32, 500)      9000        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 1000)     0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32, 500)      3002000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 32, 500)      2002000     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "offset (TimeDistributed)        (None, 32, 1001)     501501      lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "iosize (TimeDistributed)        (None, 32, 18)       9018        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,024,019\n",
      "Trainable params: 6,024,019\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Two classification outputs\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate , Dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed, Reshape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# no_docs = len(y_train_lba)\n",
    "maxlen= 32\n",
    "\n",
    "\n",
    "\n",
    "# # define two sets of inputs\n",
    "# inputA = Input(shape=(32,))\n",
    "# inputB = Input(shape=(32,))\n",
    "# # inputA = Sequential()\n",
    "# # inputB = Sequential()\n",
    "vocabulary_1 = 1001\n",
    "vocabulary_2 = len(Counter(df['Size_class']))\n",
    "\n",
    "hidden_size = 500\n",
    "\n",
    "# input=Input(shape=(no_docs,maxlen),dtype='float64')\n",
    "inputA=Input(shape=(maxlen,),dtype='float64')  \n",
    "inputB=Input(shape=(maxlen,),dtype='float64') \n",
    "\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Embedding(input_dim=vocabulary_1,output_dim=hidden_size,input_length=maxlen)(inputA)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# # the second branch opreates on the second input\n",
    "y = Embedding(input_dim=vocabulary_2,output_dim=hidden_size,input_length=maxlen)(inputB)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "# combine the output of the two branches\n",
    "combined = keras.layers.concatenate([x.output, y.output])\n",
    "\n",
    "lstm1 = LSTM(hidden_size,return_sequences=True)(combined)\n",
    "lstm2 = LSTM(hidden_size, return_sequences=True)(lstm1)\n",
    "\n",
    "# create classification output\n",
    "offset = keras.layers.wrappers.TimeDistributed(Dense(units=vocabulary_1, activation='softmax'), name='offset')(lstm2)\n",
    "iosize = keras.layers.wrappers.TimeDistributed(Dense(units=vocabulary_2, activation='softmax'), name='iosize')(lstm2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model =Model([inputA,inputB],[offset,iosize]) # combining all into a Keras model\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'offset': 'sparse_categorical_crossentropy', 'iosize': 'sparse_categorical_crossentropy'},\n",
    "              loss_weights={'offset': 2., 'iosize': 1.5},\n",
    "              metrics={ 'offset': 'categorical_accuracy', 'iosize': 'categorical_accuracy'})\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soe/hlitz/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200257 samples, validate on 400043 samples\n",
      "Epoch 1/1000\n",
      "1200257/1200257 [==============================] - 1991s 2ms/step - loss: 3.0795 - offset_loss: 0.5057 - iosize_loss: 1.3788 - offset_categorical_accuracy: 0.9983 - iosize_categorical_accuracy: 0.6620 - val_loss: 3.2603 - val_offset_loss: 0.5628 - val_iosize_loss: 1.4231 - val_offset_categorical_accuracy: 0.9966 - val_iosize_categorical_accuracy: 0.6638\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.26029, saving model to best_weights-buildserver-2.hdf5\n",
      "Epoch 2/1000\n",
      "1200257/1200257 [==============================] - 1998s 2ms/step - loss: 2.8313 - offset_loss: 0.4302 - iosize_loss: 1.3139 - offset_categorical_accuracy: 0.9853 - iosize_categorical_accuracy: 0.6405 - val_loss: 3.5723 - val_offset_loss: 0.6478 - val_iosize_loss: 1.5177 - val_offset_categorical_accuracy: 0.9877 - val_iosize_categorical_accuracy: 0.5295\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.26029\n",
      "Epoch 3/1000\n",
      "1200257/1200257 [==============================] - 1981s 2ms/step - loss: 2.5675 - offset_loss: 0.3683 - iosize_loss: 1.2206 - offset_categorical_accuracy: 0.9740 - iosize_categorical_accuracy: 0.6126 - val_loss: 3.9793 - val_offset_loss: 0.7479 - val_iosize_loss: 1.6556 - val_offset_categorical_accuracy: 0.9723 - val_iosize_categorical_accuracy: 0.4431\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 3.26029\n",
      "Epoch 4/1000\n",
      "1200257/1200257 [==============================] - 2000s 2ms/step - loss: 2.3872 - offset_loss: 0.3348 - iosize_loss: 1.1450 - offset_categorical_accuracy: 0.9678 - iosize_categorical_accuracy: 0.5933 - val_loss: 4.1435 - val_offset_loss: 0.7507 - val_iosize_loss: 1.7613 - val_offset_categorical_accuracy: 0.9781 - val_iosize_categorical_accuracy: 0.4264\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3.26029\n",
      "Epoch 5/1000\n",
      "1200257/1200257 [==============================] - 1993s 2ms/step - loss: 2.2664 - offset_loss: 0.3147 - iosize_loss: 1.0913 - offset_categorical_accuracy: 0.9641 - iosize_categorical_accuracy: 0.5807 - val_loss: 4.4014 - val_offset_loss: 0.7923 - val_iosize_loss: 1.8777 - val_offset_categorical_accuracy: 0.9743 - val_iosize_categorical_accuracy: 0.4998\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 3.26029\n",
      "Epoch 6/1000\n",
      "1200257/1200257 [==============================] - 1992s 2ms/step - loss: 2.1801 - offset_loss: 0.3019 - iosize_loss: 1.0509 - offset_categorical_accuracy: 0.9614 - iosize_categorical_accuracy: 0.5713 - val_loss: 4.6085 - val_offset_loss: 0.8298 - val_iosize_loss: 1.9659 - val_offset_categorical_accuracy: 0.9647 - val_iosize_categorical_accuracy: 0.5107\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 3.26029\n",
      "Epoch 7/1000\n",
      "1200257/1200257 [==============================] - 1996s 2ms/step - loss: 2.1154 - offset_loss: 0.2926 - iosize_loss: 1.0202 - offset_categorical_accuracy: 0.9598 - iosize_categorical_accuracy: 0.5643 - val_loss: 4.6806 - val_offset_loss: 0.8217 - val_iosize_loss: 2.0247 - val_offset_categorical_accuracy: 0.9710 - val_iosize_categorical_accuracy: 0.4548\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.26029\n",
      "Epoch 8/1000\n",
      "1200257/1200257 [==============================] - 1993s 2ms/step - loss: 2.0619 - offset_loss: 0.2850 - iosize_loss: 0.9946 - offset_categorical_accuracy: 0.9585 - iosize_categorical_accuracy: 0.5581 - val_loss: 4.7560 - val_offset_loss: 0.8333 - val_iosize_loss: 2.0595 - val_offset_categorical_accuracy: 0.9729 - val_iosize_categorical_accuracy: 0.4326\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 3.26029\n",
      "Epoch 9/1000\n",
      "1200257/1200257 [==============================] - 2001s 2ms/step - loss: 2.0168 - offset_loss: 0.2786 - iosize_loss: 0.9730 - offset_categorical_accuracy: 0.9572 - iosize_categorical_accuracy: 0.5527 - val_loss: 4.8559 - val_offset_loss: 0.8490 - val_iosize_loss: 2.1051 - val_offset_categorical_accuracy: 0.9658 - val_iosize_categorical_accuracy: 0.5005\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.26029\n",
      "Epoch 10/1000\n",
      "1200257/1200257 [==============================] - 1986s 2ms/step - loss: 1.9806 - offset_loss: 0.2738 - iosize_loss: 0.9554 - offset_categorical_accuracy: 0.9565 - iosize_categorical_accuracy: 0.5479 - val_loss: 4.9266 - val_offset_loss: 0.8686 - val_iosize_loss: 2.1261 - val_offset_categorical_accuracy: 0.9649 - val_iosize_categorical_accuracy: 0.4710\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.26029\n",
      "Epoch 11/1000\n",
      "1200257/1200257 [==============================] - 2003s 2ms/step - loss: 1.9526 - offset_loss: 0.2705 - iosize_loss: 0.9410 - offset_categorical_accuracy: 0.9560 - iosize_categorical_accuracy: 0.5439 - val_loss: 4.9780 - val_offset_loss: 0.8472 - val_iosize_loss: 2.1889 - val_offset_categorical_accuracy: 0.9749 - val_iosize_categorical_accuracy: 0.4975\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 3.26029\n",
      "Epoch 12/1000\n",
      "1200257/1200257 [==============================] - 1992s 2ms/step - loss: 1.9276 - offset_loss: 0.2676 - iosize_loss: 0.9283 - offset_categorical_accuracy: 0.9556 - iosize_categorical_accuracy: 0.5402 - val_loss: 4.9856 - val_offset_loss: 0.8437 - val_iosize_loss: 2.1986 - val_offset_categorical_accuracy: 0.9739 - val_iosize_categorical_accuracy: 0.4471\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.26029\n",
      "Epoch 13/1000\n",
      "1200257/1200257 [==============================] - 1988s 2ms/step - loss: 1.9043 - offset_loss: 0.2649 - iosize_loss: 0.9164 - offset_categorical_accuracy: 0.9552 - iosize_categorical_accuracy: 0.5377 - val_loss: 4.9702 - val_offset_loss: 0.8404 - val_iosize_loss: 2.1928 - val_offset_categorical_accuracy: 0.9766 - val_iosize_categorical_accuracy: 0.4875\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.26029\n",
      "Epoch 14/1000\n",
      " 910208/1200257 [=====================>........] - ETA: 6:45 - loss: 1.8855 - offset_loss: 0.2630 - iosize_loss: 0.9063 - offset_categorical_accuracy: 0.9547 - iosize_categorical_accuracy: 0.5349"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200257/1200257 [==============================] - 1991s 2ms/step - loss: 1.8865 - offset_loss: 0.2634 - iosize_loss: 0.9065 - offset_categorical_accuracy: 0.9548 - iosize_categorical_accuracy: 0.5348 - val_loss: 5.0604 - val_offset_loss: 0.8560 - val_iosize_loss: 2.2322 - val_offset_categorical_accuracy: 0.9645 - val_iosize_categorical_accuracy: 0.4453\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.26029\n",
      "Epoch 15/1000\n",
      "  67264/1200257 [>.............................] - ETA: 26:22 - loss: 1.8595 - offset_loss: 0.2593 - iosize_loss: 0.8940 - offset_categorical_accuracy: 0.9547 - iosize_categorical_accuracy: 0.5329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 425536/1200257 [=========>....................] - ETA: 17:59 - loss: 1.8668 - offset_loss: 0.2611 - iosize_loss: 0.8964 - offset_categorical_accuracy: 0.9545 - iosize_categorical_accuracy: 0.5330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 787584/1200257 [==================>...........] - ETA: 9:33 - loss: 1.8696 - offset_loss: 0.2621 - iosize_loss: 0.8969 - offset_categorical_accuracy: 0.9545 - iosize_categorical_accuracy: 0.5332"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144896/1200257 [===========================>..] - ETA: 1:16 - loss: 1.8699 - offset_loss: 0.2622 - iosize_loss: 0.8970 - offset_categorical_accuracy: 0.9545 - iosize_categorical_accuracy: 0.5329"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 303872/1200257 [======>.......................] - ETA: 20:47 - loss: 1.8445 - offset_loss: 0.2580 - iosize_loss: 0.8856 - offset_categorical_accuracy: 0.9541 - iosize_categorical_accuracy: 0.5304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 465856/1200257 [==========>...................] - ETA: 17:00 - loss: 1.8483 - offset_loss: 0.2590 - iosize_loss: 0.8869 - offset_categorical_accuracy: 0.9541 - iosize_categorical_accuracy: 0.5303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 664320/1200257 [===============>..............] - ETA: 12:22 - loss: 1.8499 - offset_loss: 0.2592 - iosize_loss: 0.8877 - offset_categorical_accuracy: 0.9541 - iosize_categorical_accuracy: 0.5303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 825536/1200257 [===================>..........] - ETA: 8:38 - loss: 1.8512 - offset_loss: 0.2597 - iosize_loss: 0.8879 - offset_categorical_accuracy: 0.9541 - iosize_categorical_accuracy: 0.5305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025280/1200257 [========================>.....] - ETA: 4:01 - loss: 1.8523 - offset_loss: 0.2600 - iosize_loss: 0.8882 - offset_categorical_accuracy: 0.9541 - iosize_categorical_accuracy: 0.5304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1187264/1200257 [============================>.] - ETA: 17s - loss: 1.8527 - offset_loss: 0.2602 - iosize_loss: 0.8882 - offset_categorical_accuracy: 0.9542 - iosize_categorical_accuracy: 0.5303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 186752/1200257 [===>..........................] - ETA: 23:32 - loss: 1.8306 - offset_loss: 0.2576 - iosize_loss: 0.8770 - offset_categorical_accuracy: 0.9535 - iosize_categorical_accuracy: 0.5287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 348544/1200257 [=======>......................] - ETA: 19:47 - loss: 1.8323 - offset_loss: 0.2573 - iosize_loss: 0.8785 - offset_categorical_accuracy: 0.9537 - iosize_categorical_accuracy: 0.5284"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 548736/1200257 [============>.................] - ETA: 15:08 - loss: 1.8351 - offset_loss: 0.2577 - iosize_loss: 0.8797 - offset_categorical_accuracy: 0.9539 - iosize_categorical_accuracy: 0.5287"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 710080/1200257 [================>.............] - ETA: 11:21 - loss: 1.8358 - offset_loss: 0.2580 - iosize_loss: 0.8798 - offset_categorical_accuracy: 0.9539 - iosize_categorical_accuracy: 0.5285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 911040/1200257 [=====================>........] - ETA: 6:41 - loss: 1.8369 - offset_loss: 0.2585 - iosize_loss: 0.8799 - offset_categorical_accuracy: 0.9539 - iosize_categorical_accuracy: 0.5283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071296/1200257 [=========================>....] - ETA: 2:59 - loss: 1.8370 - offset_loss: 0.2586 - iosize_loss: 0.8800 - offset_categorical_accuracy: 0.9538 - iosize_categorical_accuracy: 0.5281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200257/1200257 [==============================] - 1975s 2ms/step - loss: 1.8378 - offset_loss: 0.2587 - iosize_loss: 0.8803 - offset_categorical_accuracy: 0.9539 - iosize_categorical_accuracy: 0.5280 - val_loss: 5.0766 - val_offset_loss: 0.8495 - val_iosize_loss: 2.2516 - val_offset_categorical_accuracy: 0.9710 - val_iosize_categorical_accuracy: 0.5179\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.26029\n",
      "Epoch 18/1000\n",
      "  73024/1200257 [>.............................] - ETA: 26:13 - loss: 1.8076 - offset_loss: 0.2537 - iosize_loss: 0.8668 - offset_categorical_accuracy: 0.9535 - iosize_categorical_accuracy: 0.5254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 227840/1200257 [====>.........................] - ETA: 22:23 - loss: 1.8163 - offset_loss: 0.2554 - iosize_loss: 0.8703 - offset_categorical_accuracy: 0.9533 - iosize_categorical_accuracy: 0.5245"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 429632/1200257 [=========>....................] - ETA: 17:46 - loss: 1.8193 - offset_loss: 0.2561 - iosize_loss: 0.8714 - offset_categorical_accuracy: 0.9534 - iosize_categorical_accuracy: 0.5249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 585216/1200257 [=============>................] - ETA: 14:12 - loss: 1.8202 - offset_loss: 0.2561 - iosize_loss: 0.8720 - offset_categorical_accuracy: 0.9536 - iosize_categorical_accuracy: 0.5253"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 787456/1200257 [==================>...........] - ETA: 9:32 - loss: 1.8217 - offset_loss: 0.2565 - iosize_loss: 0.8724 - offset_categorical_accuracy: 0.9536 - iosize_categorical_accuracy: 0.5255"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 944832/1200257 [======================>.......] - ETA: 5:54 - loss: 1.8224 - offset_loss: 0.2567 - iosize_loss: 0.8727 - offset_categorical_accuracy: 0.9537 - iosize_categorical_accuracy: 0.5253"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144960/1200257 [===========================>..] - ETA: 1:16 - loss: 1.8235 - offset_loss: 0.2569 - iosize_loss: 0.8731 - offset_categorical_accuracy: 0.9537 - iosize_categorical_accuracy: 0.5255"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  36736/1200257 [..............................] - ETA: 27:03 - loss: 1.7980 - offset_loss: 0.2537 - iosize_loss: 0.8604 - offset_categorical_accuracy: 0.9537 - iosize_categorical_accuracy: 0.5233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 306432/1200257 [======>.......................] - ETA: 21:28 - loss: 1.8062 - offset_loss: 0.2543 - iosize_loss: 0.8651 - offset_categorical_accuracy: 0.9536 - iosize_categorical_accuracy: 0.5238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 395200/1200257 [========>.....................] - ETA: 19:47 - loss: 1.8070 - offset_loss: 0.2545 - iosize_loss: 0.8654 - offset_categorical_accuracy: 0.9536 - iosize_categorical_accuracy: 0.5238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 666880/1200257 [===============>..............] - ETA: 13:29 - loss: 1.8113 - offset_loss: 0.2555 - iosize_loss: 0.8669 - offset_categorical_accuracy: 0.9536 - iosize_categorical_accuracy: 0.5240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 754176/1200257 [=================>............] - ETA: 11:20 - loss: 1.8122 - offset_loss: 0.2559 - iosize_loss: 0.8670 - offset_categorical_accuracy: 0.9536 - iosize_categorical_accuracy: 0.5240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059328/1200257 [=========================>....] - ETA: 3:36 - loss: 1.8148 - offset_loss: 0.2566 - iosize_loss: 0.8677 - offset_categorical_accuracy: 0.9536 - iosize_categorical_accuracy: 0.5238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 214144/1200257 [====>.........................] - ETA: 26:04 - loss: 1.7957 - offset_loss: 0.2542 - iosize_loss: 0.8582 - offset_categorical_accuracy: 0.9531 - iosize_categorical_accuracy: 0.5204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 569728/1200257 [=============>................] - ETA: 16:33 - loss: 1.8003 - offset_loss: 0.2543 - iosize_loss: 0.8611 - offset_categorical_accuracy: 0.9535 - iosize_categorical_accuracy: 0.5224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 925120/1200257 [======================>.......] - ETA: 7:13 - loss: 1.8038 - offset_loss: 0.2553 - iosize_loss: 0.8622 - offset_categorical_accuracy: 0.9534 - iosize_categorical_accuracy: 0.5223"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200257/1200257 [==============================] - 2304s 2ms/step - loss: 1.8049 - offset_loss: 0.2556 - iosize_loss: 0.8625 - offset_categorical_accuracy: 0.9534 - iosize_categorical_accuracy: 0.5223 - val_loss: 5.2545 - val_offset_loss: 0.8667 - val_iosize_loss: 2.3473 - val_offset_categorical_accuracy: 0.9695 - val_iosize_categorical_accuracy: 0.4673\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.26029\n",
      "Epoch 21/1000\n",
      "1200257/1200257 [==============================] - 2342s 2ms/step - loss: 1.7960 - offset_loss: 0.2548 - iosize_loss: 0.8576 - offset_categorical_accuracy: 0.9533 - iosize_categorical_accuracy: 0.5210 - val_loss: 5.2720 - val_offset_loss: 0.8657 - val_iosize_loss: 2.3603 - val_offset_categorical_accuracy: 0.9707 - val_iosize_categorical_accuracy: 0.4997\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 3.26029\n",
      "Epoch 22/1000\n",
      " 380160/1200257 [========>.....................] - ETA: 22:01 - loss: 1.7807 - offset_loss: 0.2522 - iosize_loss: 0.8508 - offset_categorical_accuracy: 0.9531 - iosize_categorical_accuracy: 0.5199"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-036a259361cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m model.fit([X_train_lba,X_train_size],[y_train_lba,y_train_size],\n\u001b[0;32m---> 14\u001b[0;31m           verbose=1,epochs=1000,callbacks=[monitor,checkpointer],batch_size = 64,validation_data = valid)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "monitor = EarlyStopping(monitor='loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights-buildserver-2.hdf5\", verbose=1, save_best_only=True) # save best model\n",
    "valid = [X_test_lba,X_test_size],[y_test_lba,y_test_size]\n",
    "\n",
    "\n",
    "valid = [X_test_lba,X_test_size],[y_test_lba,y_test_size]\n",
    "\n",
    "model.fit([X_train_lba,X_train_size],[y_train_lba,y_train_size],\n",
    "          verbose=1,epochs=1000,callbacks=[monitor,checkpointer],batch_size = 64,validation_data = valid)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print('Done ...!')\n",
    "\n",
    "#model.fit(X_train, y_train, nb_epoch = num_epochs, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/Model_checkpoints/buildserver-2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soe/hlitz/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "new_model = keras.models.load_model('/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/Model_checkpoints/buildserver-2_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400043/400043 [==============================] - 579s 1ms/step\n",
      "579.3955948352814\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pred1,pred2 = new_model.predict([X_test_lba,X_test_size],verbose =1 )\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.65558934211731\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pred_1 = pred1[:,0,:]\n",
    "pred_2 = pred2[:,0,:]\n",
    "pred_1 = np.argmax(pred_1, axis=1)\n",
    "pred_2 = np.argmax(pred_2, axis=1)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train, validate and test\n",
    "\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "lba_train =df[:training_pt_1]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "lba_test = df[training_pt_1+1:]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "size_train = df[:training_pt_1]['Size_class'].tolist()\n",
    "size_test = df[training_pt_1+1:]['Size_class'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400107"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400043\n",
      "400043\n"
     ]
    }
   ],
   "source": [
    "print(len(pred1))\n",
    "lba_test = lba_test[(-len(pred1)):]\n",
    "print(len(lba_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lba_test_final = lba_test[(-len(pred1)):]\n",
    "lba_size_final = size_test[(-len(pred1)):]\n",
    "\n",
    "\n",
    "lba_accuracy = accuracy_score(lba_test_final, pred_1)\n",
    "\n",
    "\n",
    "size_accuracy = accuracy_score(lba_size_final, pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9217834082836095\n",
      "0.3561542134220571\n"
     ]
    }
   ],
   "source": [
    "print(lba_accuracy)\n",
    "print(size_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_tf_gpu] *",
   "language": "python",
   "name": "conda-env-py3_tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
