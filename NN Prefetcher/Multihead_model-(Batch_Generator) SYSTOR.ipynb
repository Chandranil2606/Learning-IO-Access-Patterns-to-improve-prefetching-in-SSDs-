{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/aakashkapoor103/anaconda3/Data/2016030817-LUN2.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "path  = r\"/home/aakashkapoor103/anaconda3/Data/\" \n",
    "\n",
    "names = ['Timestamp','Response','IOType','LUN','ByteOffset','IOSize']\n",
    "\n",
    "#['TimeStamp','Host_Name','DiskNumber','Operation_Type','ByteOffset','IOSize','Response_Time','DiskNum']\n",
    "\n",
    "#['TimeStamp','Response','IOType','LUN','ByteOffset','Size']\n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"2016030817-LUN2.csv\"))\n",
    "print(all_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4443486\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(all_files[0],engine='python',skiprows =2,header=None,na_values=['-1'], index_col=False) \n",
    "names = names = ['Timestamp','Response','IOType','LUN','ByteOffset','IOSize']\n",
    "df.columns = names\n",
    "print (len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp     float64\n",
      "Response      float64\n",
      "IOType         object\n",
      "LUN             int64\n",
      "ByteOffset      int64\n",
      "IOSize          int64\n",
      "dtype: object\n",
      "4443486\n"
     ]
    }
   ],
   "source": [
    "#Sorting df by TimeStamp\n",
    "\n",
    "df = df.sort_values(by=['Timestamp'])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(df.dtypes)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "998118\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(Counter(df['IOSize'])))\n",
    "print(len(Counter(df['ByteOffset'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ByteOffset_Delta'] = df['ByteOffset'] - df['ByteOffset'].shift(-1)\n",
    "df = df.drop(df.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "\n",
    "a = df['ByteOffset_Delta'].unique().tolist()\n",
    "operation_id_map = {}\n",
    "for i,id in enumerate(a): operation_id_map[id] = i \n",
    "df['ByteOffset_Delta_class'] = df['ByteOffset_Delta'].map(lambda x: operation_id_map[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Response</th>\n",
       "      <th>IOType</th>\n",
       "      <th>LUN</th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>IOSize</th>\n",
       "      <th>ByteOffset_Delta</th>\n",
       "      <th>ByteOffset_Delta_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.457424e+09</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>W</td>\n",
       "      <td>2</td>\n",
       "      <td>843332589056</td>\n",
       "      <td>8192</td>\n",
       "      <td>-4.286630e+12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.457424e+09</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>R</td>\n",
       "      <td>2</td>\n",
       "      <td>5129962784768</td>\n",
       "      <td>32768</td>\n",
       "      <td>5.370235e+11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.457424e+09</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>W</td>\n",
       "      <td>2</td>\n",
       "      <td>4592939282432</td>\n",
       "      <td>8192</td>\n",
       "      <td>-5.370235e+11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.457424e+09</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>R</td>\n",
       "      <td>2</td>\n",
       "      <td>5129962817536</td>\n",
       "      <td>32768</td>\n",
       "      <td>5.370235e+11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.457424e+09</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>W</td>\n",
       "      <td>2</td>\n",
       "      <td>4592939290624</td>\n",
       "      <td>8192</td>\n",
       "      <td>3.749658e+12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Timestamp  Response IOType  LUN     ByteOffset  IOSize  \\\n",
       "0  1.457424e+09  0.000133      W    2   843332589056    8192   \n",
       "1  1.457424e+09  0.008872      R    2  5129962784768   32768   \n",
       "2  1.457424e+09  0.000188      W    2  4592939282432    8192   \n",
       "3  1.457424e+09  0.003448      R    2  5129962817536   32768   \n",
       "4  1.457424e+09  0.000141      W    2  4592939290624    8192   \n",
       "\n",
       "   ByteOffset_Delta  ByteOffset_Delta_class  \n",
       "0     -4.286630e+12                       0  \n",
       "1      5.370235e+11                       1  \n",
       "2     -5.370235e+11                       2  \n",
       "3      5.370235e+11                       3  \n",
       "4      3.749658e+12                       4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "x = Counter(df['ByteOffset_Delta_class'])\n",
    "vals = {}\n",
    "vals =  x.most_common(1000)\n",
    "bo_list = []\n",
    "\n",
    "for x in vals:\n",
    "    bo_list.append(x[0])\n",
    "        \n",
    "count = 0\n",
    "label_list = []\n",
    "\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset_Delta_class'].iloc[count]\n",
    "    if x in bo_list:\n",
    "        label_list.append(x)\n",
    "    else:\n",
    "        label_list.append(999999)\n",
    "    count= count + 1\n",
    "    \n",
    "ByteOffset_Delta_class_backup  = df['ByteOffset_Delta_class'] \n",
    "df['ByteOffset_Delta_class']  = label_list\n",
    "print(len(Counter(df['ByteOffset_Delta_class'])))\n",
    "    \n",
    "a = df['ByteOffset_Delta_class'].unique().tolist()\n",
    "bo_map = {}\n",
    "for i,id in enumerate(a): bo_map[id] = i \n",
    "df['ByteOffset_Delta_Class_1001'] = df['ByteOffset_Delta_class'].map(lambda x: bo_map[x])\n",
    "    \n",
    "label_list = df['ByteOffset_Delta_Class_1001'] \n",
    "    \n",
    "df['ByteOffset_Delta_Class_1001']  = label_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IOSize_log'] = np.log2(df['IOSize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(df['ByteOffset_Delta_Class_1001']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "259\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['IOSize_log'] = np.log2(df['IOSize'])\n",
    "df['IOSize_log_roundoff']= round(df['IOSize_log'])\n",
    "print(len(Counter(df['IOSize'])))\n",
    "print(len(Counter(df['IOSize_log'])))\n",
    "print(len(Counter(df['IOSize_log_roundoff'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['IOSize_log_roundoff'].unique().tolist()\n",
    "size_id_map = {}\n",
    "for i,id in enumerate(a): size_id_map[id] = i \n",
    "df['Size_Class'] = df['IOSize_log_roundoff'].map(lambda x: size_id_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 631161,\n",
       "         1: 320962,\n",
       "         2: 2867587,\n",
       "         3: 71171,\n",
       "         4: 58650,\n",
       "         5: 437353,\n",
       "         6: 30534,\n",
       "         7: 13797,\n",
       "         8: 8980,\n",
       "         9: 3290})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['Size_Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp                      float64\n",
       "Response                       float64\n",
       "IOType                          object\n",
       "LUN                              int64\n",
       "ByteOffset                       int64\n",
       "IOSize                           int64\n",
       "ByteOffset_Delta               float64\n",
       "ByteOffset_Delta_class           int64\n",
       "ByteOffset_Delta_Class_1001      int64\n",
       "IOSize_log                     float64\n",
       "IOSize_log_roundoff            float64\n",
       "Size_Class                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop: Index(['Timestamp', 'Response', 'IOType', 'LUN', 'ByteOffset', 'IOSize',\n",
      "       'ByteOffset_Delta', 'ByteOffset_Delta_class',\n",
      "       'ByteOffset_Delta_Class_1001', 'IOSize_log', 'IOSize_log_roundoff',\n",
      "       'Size_Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary coloumns\n",
    "\n",
    "print(\"Before drop: {}\".format(df.columns))\n",
    "df.drop(df.columns[[0,1,2,3,4,5,6,7,9,10]], axis=1,inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteOffset_Delta_Class_1001    int64\n",
       "Size_Class                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(Counter(df['Size_Class'])))\n",
    "from collections import Counter\n",
    "print(len(Counter(df['ByteOffset_Delta_Class_1001'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train, validate and test\n",
    "\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "lba_train =df[:training_pt_1]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "lba_test = df[training_pt_1+1:]['ByteOffset_Delta_Class_1001'].tolist()\n",
    "size_train = df[:training_pt_1]['Size_Class'].tolist()\n",
    "size_test = df[training_pt_1+1:]['Size_Class'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = r'/home/aakashkapoor103/anaconda3/Data/files/'\n",
    "\n",
    "path_train  = r\"/home/aakashkapoor103/anaconda3/Data/files/lba_train_2016030817.txt\"\n",
    "\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in lba_train:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n",
    "path_test  = r\"/home/aakashkapoor103/anaconda3/Data/files/lba_test_2016030817.txt\"\n",
    "with open(path_test, 'w') as f:\n",
    "    for item in lba_test:\n",
    "        f.write(\"%s \" % item)\n",
    "\n",
    "        \n",
    "path_train  = r\"/home/aakashkapoor103/anaconda3/Data/files/size_train_2016030817.txt\"\n",
    "\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in size_train:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n",
    "path_test  = r\"/home/aakashkapoor103/anaconda3/Data/files/size_test_2016030817.txt\"\n",
    "with open(path_test, 'w') as f:\n",
    "    for item in size_test:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "data_path  = r'/home/aakashkapoor103/anaconda3/Data/files/'\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('run_opt', type=int, default=1, help='An integer: 1 to train, 2 to test')\n",
    "# parser.add_argument('--data_path', type=str, default=data_path, help='The full path of the training data')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# if args.data_path:\n",
    "#     data_path = args.data_path\n",
    "\n",
    "\n",
    "# def read_words(filename):\n",
    "#     with tf.gfile.GFile(filename, \"r\") as f:\n",
    "#         return f.read().split()\n",
    "\n",
    "def read_words(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().split()\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "\n",
    "def load_data_lba():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"lba_train_2016030817.txt\")\n",
    "    test_path = os.path.join(data_path, \"lba_test_2016030817.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    #print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, test_data, vocabulary, reversed_dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data_size():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"size_train_2016031115.txt\")\n",
    "    test_path = os.path.join(data_path, \"size_test_2016031115.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    #print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return np.array(train_data), np.array(test_data), vocabulary, reversed_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "1001\n",
      "0 0 0 0 0 0 0 1 1 1\n",
      "[0, 0, 0, 7, 0]\n",
      "11\n",
      "0 0 0 1 0 2 0 3 0 3\n"
     ]
    }
   ],
   "source": [
    "train_data_lba, test_data_lba, vocabulary_1, reversed_dictionary_1 = load_data_lba()\n",
    "train_data_size, test_data_size, vocabulary_2, reversed_dictionary_2 = load_data_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 32, 500)      500500      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 32, 500)      5000        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 1000)     0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32, 500)      3002000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 32, 500)      2002000     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "offset (TimeDistributed)        (None, 32, 1001)     501501      lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "iosize (TimeDistributed)        (None, 32, 10)       5010        lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,016,011\n",
      "Trainable params: 6,016,011\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Two classification outputs\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate , Dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed, Reshape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# no_docs = len(y_train_lba)\n",
    "maxlen= 32\n",
    "\n",
    "# from collections import Counter\n",
    "# print(len(Counter(df['Size_Class'])))\n",
    "# from collections import Counter\n",
    "# print(len(Counter(df['ByteOffset_Delta_Class_1001'])))\n",
    "\n",
    "# # define two sets of inputs\n",
    "# inputA = Input(shape=(32,))\n",
    "# inputB = Input(shape=(32,))\n",
    "# # inputA = Sequential()\n",
    "# # inputB = Sequential()\n",
    "vocabulary_1 = len(Counter(df['ByteOffset_Delta_Class_1001']))\n",
    "vocabulary_2 = len(Counter(df['Size_Class']))\n",
    "hidden_size = 500\n",
    "\n",
    "# input=Input(shape=(no_docs,maxlen),dtype='float64')\n",
    "inputA=Input(shape=(maxlen,),dtype='float64')  \n",
    "inputB=Input(shape=(maxlen,),dtype='float64') \n",
    "\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x = Embedding(input_dim=vocabulary_1,output_dim=hidden_size,input_length=maxlen)(inputA)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# # the second branch opreates on the second input\n",
    "y = Embedding(input_dim=vocabulary_2,output_dim=hidden_size,input_length=maxlen)(inputB)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "# combine the output of the two branches\n",
    "combined = keras.layers.concatenate([x.output, y.output])\n",
    "\n",
    "lstm1 = LSTM(hidden_size,return_sequences=True)(combined)\n",
    "lstm2 = LSTM(hidden_size, return_sequences=True)(lstm1)\n",
    "\n",
    "# create classification output\n",
    "offset = keras.layers.wrappers.TimeDistributed(Dense(units=vocabulary_1, activation='softmax'), name='offset')(lstm2)\n",
    "iosize = keras.layers.wrappers.TimeDistributed(Dense(units=vocabulary_2, activation='softmax'), name='iosize')(lstm2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model =Model([inputA,inputB],[offset,iosize]) # combining all into a Keras model\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'offset': 'categorical_crossentropy', 'iosize': 'categorical_crossentropy'},\n",
    "              loss_weights={'offset': 2., 'iosize': 1.5},\n",
    "              metrics={ 'offset': 'categorical_accuracy', 'iosize': 'categorical_accuracy'})\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data_1, data_2, num_steps, batch_size, vocabulary1,vocabulary2, skip_step=0):\n",
    "        self.data_1 = data_1\n",
    "        self.data_2 = data_2\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary1 = vocabulary1\n",
    "        self.vocabulary2 = vocabulary2\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x_lba = np.zeros((self.batch_size, self.num_steps))\n",
    "        x_size = np.zeros((self.batch_size, self.num_steps))\n",
    "        y_lba = np.zeros((self.batch_size, self.num_steps, self.vocabulary1))\n",
    "        y_size = np.zeros((self.batch_size, self.num_steps, self.vocabulary2))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                predict_ahead = 32\n",
    "                if self.current_idx + self.num_steps >= len(self.data_1):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x_lba[i, :] = self.data_1[self.current_idx:self.current_idx + self.num_steps]\n",
    "                x_size[i, :] = self.data_2[self.current_idx:self.current_idx + self.num_steps]\n",
    "                \n",
    "                temp_y_lba = self.data_1[self.current_idx + predict_ahead:self.current_idx + self.num_steps + 1]\n",
    "                temp_y_size = self.data_2[self.current_idx + predict_ahead:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y_lba[i, :, :] = to_categorical(temp_y_lba, num_classes=self.vocabulary1)\n",
    "                y_size[i, :, :] = to_categorical(temp_y_size, num_classes=self.vocabulary2)\n",
    "                self.current_idx += self.skip_step\n",
    "                \n",
    "            #yield (np.array(x_lba),np.array(x_size)), (np.array(y_lba),np.array(y_size))\n",
    "            yield [x_lba, x_size],[y_lba, y_size]\n",
    "#            yield [np.array(top_batch), np.array(bot_batch)], np.array(batch_labels)\n",
    "#             print(x)\n",
    "#             print(y)\n",
    "#             yield x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 32\n",
    "batch_size = 32\n",
    "\n",
    "train_data_generator = KerasBatchGenerator(train_data_lba,train_data_size, num_steps, batch_size, vocabulary_1,vocabulary_2,skip_step=0) \n",
    "test_data_generator = KerasBatchGenerator(test_data_lba,test_data_size, num_steps, batch_size, vocabulary_1, vocabulary_2, skip_step=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aakashkapoor103/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3254/3254 [==============================] - 322s 99ms/step - loss: 0.0136 - offset_loss: 0.0059 - iosize_loss: 0.0012 - offset_categorical_accuracy: 0.9997 - iosize_categorical_accuracy: 0.9997 - val_loss: 55.0349 - val_offset_loss: 15.8309 - val_iosize_loss: 15.5823 - val_offset_categorical_accuracy: 0.0000e+00 - val_iosize_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 55.03490, saving model to /home/aakashkapoor103/anaconda3/Data/files/best_weights-4.hdf5\n",
      "Epoch 2/1000\n",
      "2606/3254 [=======================>......] - ETA: 43s - loss: 4.1723e-07 - offset_loss: 1.1921e-07 - iosize_loss: 1.1921e-07 - offset_categorical_accuracy: 1.0000 - iosize_categorical_accuracy: 1.0000 E"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "num_steps = 32\n",
    "\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + 'best_weights-4.hdf5', verbose=1,save_best_only=True)\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "# model.fit([X_train_lba,X_train_size],[y_train_lba,y_train_size],\n",
    "#           verbose=1,epochs=1000,callbacks=[monitor,checkpointer])\n",
    "\n",
    "valid_steps = len(train_data_lba)//(batch_size* 32)\n",
    "train_steps = len(train_data_lba)//(batch_size *32)\n",
    "\n",
    "model.fit_generator(train_data_generator.generate(),\n",
    "                    train_steps,\n",
    "                    num_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_data_generator.generate(),\n",
    "                    validation_steps= valid_steps,\n",
    "                    callbacks=[checkpointer,monitor])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "batch_size = 32\n",
    "samples = (len(size_test)/batch_size)\n",
    "score = model.evaluate_generator(test_data_generator.generate(), samples, verbose = 1)\n",
    "print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "pickle_off = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_X_train_lba.csv\",\"rb\")\n",
    "X_train_lba = pickle.load(pickle_off)\n",
    "\n",
    "pickle_off_1 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_X_train_size.csv\",\"rb\")\n",
    "X_train_size = pickle.load(pickle_off_1)\n",
    "\n",
    "pickle_off_2 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_X_test_lba.csv\",\"rb\")\n",
    "X_test_lba = pickle.load(pickle_off_2)\n",
    "\n",
    "pickle_off_3 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_X_test_size.csv\",\"rb\")\n",
    "X_test_size = pickle.load(pickle_off_3)\n",
    "\n",
    "pickle_off_4 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_y_train_lba.csv\",\"rb\")\n",
    "y_train_lba = pickle.load(pickle_off_4)\n",
    "\n",
    "pickle_off_5 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_y_train_size.csv\",\"rb\")\n",
    "y_train_size = pickle.load(pickle_off_5)\n",
    "\n",
    "pickle_off_6 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_y_test_lba.csv\",\"rb\")\n",
    "y_test_lba = pickle.load(pickle_off_6)\n",
    "\n",
    "pickle_off_7 = open(\"/home/aakashkapoor103/anaconda3/Data/nump_checkpoints/mds0_y_test_size.csv\",\"rb\")\n",
    "y_test_size = pickle.load(pickle_off_7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lba.shape, y_train_lba.shape, X_test_lba.shape,  y_test_lba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_size.shape, y_train_size.shape, X_test_size.shape,  y_test_size.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "pred1,pred2 = model.predict([X_test_lba,X_test_size],verbose =1 )\n",
    "\n",
    "print(\"Elapsed time: {}\".format((elapsed_time)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_validation_samples = len(size_test)\n",
    "# pred_1,pred_2 = model.predict_generator(test_data_generator.generate(), nb_validation_samples // batch_size + 1, verbose = 1)\n",
    "# # y_pred_1 = np.argmax(pred_1, axis = 1)\n",
    "# # y_pred_2 = np.argmax(pred_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# precision_recall_fscore_support(test_data_lba[32:], y_pred_1, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# precision_recall_fscore_support(test_data_size[32:], y_pred_2, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# max_lba_accuracy = 0\n",
    "# max_lba_accuracy_pos = 0\n",
    "\n",
    "# max_size_accuracy = 0\n",
    "# max_size_accuracy_pos = 0\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# for i in range(32):\n",
    "#     pred_1 = pred1[:,i,:]\n",
    "#     pred_2 = pred2[:,i,:]\n",
    "#     pred_1 = np.argmax(pred_1, axis=1)\n",
    "#     pred_2 = np.argmax(pred_2, axis=1)\n",
    "    \n",
    "    \n",
    "#     tmp_lba = accuracy_score(lba_test_final, pred_1)\n",
    "#     if (tmp_lba > max_lba_accuracy):\n",
    "#         max_lba_accuracy = tmp_lba\n",
    "#         max_lba_accuracy_pos = i\n",
    "        \n",
    "#     tmp_size = accuracy_score(lba_size_final, pred_2)\n",
    "#     if (tmp_size > max_size_accuracy):\n",
    "#         max_size_accuracy = tmp_size\n",
    "#         max_size_accuracy_pos = i\n",
    "    \n",
    "\n",
    "# print(\"Best IO Size Accuracy\", str(max_size_accuracy))\n",
    "# print(\"Best IO Size Pos\", str(max_size_accuracy_pos))\n",
    "\n",
    "# print(\"Best LBA Delta Accuracy\", str(max_lba_accuracy))\n",
    "# print(\"Best LBA Delta Position\", str(max_lba_accuracy_pos))\n",
    "\n",
    "# print(\"Elapsed time: {}\".format((elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done...!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
