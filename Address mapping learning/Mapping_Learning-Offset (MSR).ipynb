{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Chandranil Chakraborttii\n",
    "# Project: Learning I/O Access Patterns to Improve Prefetching in SSDs\n",
    "# Paper Link :https://www.researchgate.net/profile/Chandranil_Chakraborttii/publication/344379801_Learning_IO_Access_Patterns_to_Improve_Prefetching_in_SSDs/links/5f6e28fba6fdcc00863adb13/Learning-I-O-Access-Patterns-to-Improve-Prefetching-in-SSDs.pdf\n",
    "# Loading libraries\n",
    "\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/MSR-Cambridge/Part2/src1_1.csv']\n"
     ]
    }
   ],
   "source": [
    "# Load all libraries \n",
    "# Load data (trace 1) \n",
    "# Map, order by frequency \n",
    "# Train model \n",
    "# Load another data (trace 2)\n",
    "# Map, order by frequency - Same as trace 1\n",
    "# Load model\n",
    "# Use the model to predict on the new trace\n",
    "# Note and compare results\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "path  = r\"/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/MSR-Cambridge/Part2/\" \n",
    "\n",
    "names = ['TimeStamp','Host_Name','DiskNumber','Operation_Type','ByteOffset','IOSize','Response_Time','DiskNum']\n",
    "\n",
    "\n",
    "#['TimeStamp','Response','IOType','LUN','ByteOffset','Size']\n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"src1_1.csv\"))\n",
    "print(all_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45746222\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(all_files[0],engine='python',skiprows =1,header=None,na_values=['-1'], index_col=False) \n",
    "names = ['TimeStamp','Host_Name','DiskNumber','Operation_Type','ByteOffset','IOSize','Response_Time','DiskNum']\n",
    "df.columns = names\n",
    "#Sorting df by TimeStamp\n",
    "\n",
    "df = df.sort_values(by=['TimeStamp'])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# df['ByteOffset_Delta'] = df['ByteOffset'] - df['ByteOffset'].shift(-1)\n",
    "# df = df.drop(df.index[-1])\n",
    "\n",
    "# total_classes = len(Counter(df['ByteOffset_Delta']))\n",
    "# x = Counter(df['ByteOffset_Delta'])\n",
    "# vals = {}\n",
    "# vals =  x.most_common(1000)\n",
    "# bo_list = []\n",
    "# coverage = 0\n",
    "# for key,value in vals:\n",
    "#     bo_list.append(key)\n",
    "#     coverage = coverage + value\n",
    "# print(len(bo_list))\n",
    "# print(\"Percentage Coverage Delta\")\n",
    "# print((coverage/len(df))*100) \n",
    "\n",
    "\n",
    "# total_classes = len(Counter(df['ByteOffset']))\n",
    "# x = Counter(df['ByteOffset'])\n",
    "# vals = {}\n",
    "# vals =  x.most_common(1000)\n",
    "# bo_list = []\n",
    "# coverage = 0\n",
    "# for key,value in vals:\n",
    "#     bo_list.append(key)\n",
    "#     coverage = coverage + value\n",
    "    \n",
    "# print(len(bo_list))\n",
    "# print(\"Percentage Coverage Offset\")\n",
    "# print((coverage/len(df))*100)\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Operation_type, Host_name and remove Disk_Num\n",
    "# Make Class of Byte Offset delta \n",
    "# Normalize the columns (Except ByteOffset Delta)\n",
    "# Make the prediction model with 1000 classes\n",
    "import operator\n",
    "     \n",
    "# df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "address_map = dict(Counter(df['ByteOffset']))\n",
    "sorted_address_map = sorted(dict(address_map).items(), key=operator.itemgetter(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3154132992, 52993)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_address_map[len(sorted_address_map)-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_remap = {}\n",
    "\n",
    "count = 0\n",
    "while(count<len(sorted_address_map)):\n",
    "    if(count < 1000):\n",
    "        address_remap[count] = sorted_address_map[len(sorted_address_map)-count - 1][0]\n",
    "    else:\n",
    "        address_remap[1001] = sorted_address_map[len(sorted_address_map)-count - 1][0]\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_address_remap = {y:x for x,y in address_remap.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Host_Name</th>\n",
       "      <th>DiskNumber</th>\n",
       "      <th>Operation_Type</th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>IOSize</th>\n",
       "      <th>Response_Time</th>\n",
       "      <th>DiskNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128166372013534666</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3180044288</td>\n",
       "      <td>65536</td>\n",
       "      <td>3899</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128166372013535022</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3337596928</td>\n",
       "      <td>65536</td>\n",
       "      <td>3543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128166372013536418</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>102050844672</td>\n",
       "      <td>4096</td>\n",
       "      <td>2148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128166372013537059</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3180109824</td>\n",
       "      <td>8192</td>\n",
       "      <td>1507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128166372021660589</td>\n",
       "      <td>src1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3180118016</td>\n",
       "      <td>28672</td>\n",
       "      <td>2925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TimeStamp Host_Name  DiskNumber Operation_Type    ByteOffset  \\\n",
       "0  128166372013534666      src1         NaN          Write    3180044288   \n",
       "1  128166372013535022      src1         NaN          Write    3337596928   \n",
       "2  128166372013536418      src1         NaN          Write  102050844672   \n",
       "3  128166372013537059      src1         NaN          Write    3180109824   \n",
       "4  128166372021660589      src1         NaN          Write    3180118016   \n",
       "\n",
       "   IOSize  Response_Time  DiskNum  \n",
       "0   65536           3899        1  \n",
       "1   65536           3543        1  \n",
       "2    4096           2148        1  \n",
       "3    8192           1507        1  \n",
       "4   28672           2925        1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "45390748\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "label_list = []\n",
    "rogue_delta = []\n",
    "label =0.0\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset'].iloc[count]\n",
    "    if x in rev_address_remap:\n",
    "        label = rev_address_remap[x]\n",
    "        label_list.append(label)\n",
    "    else:\n",
    "        label_list.append(1001)\n",
    "        rogue_delta.append(x)\n",
    "    count = count+1\n",
    "\n",
    "df['ByteOffset_class']  = label_list\n",
    "print(len(Counter(df['ByteOffset_class'])))\n",
    "print(len(rogue_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2368"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label_list)[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df['ByteOffset_class'].values.tolist()\n",
    "data_test = df['ByteOffset_class'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34309666\n",
      "11436555\n"
     ]
    }
   ],
   "source": [
    "# Split to train, validate and test\n",
    "\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "\n",
    "data_train = df['ByteOffset_class'][:training_pt_1]\n",
    "data_test = df['ByteOffset_class'][training_pt_1+1:]\n",
    "\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = r'/soe/cchakrab/test_output/output_csv/mapping/p1'\n",
    "path_train  = r\"/soe/cchakrab/test_output/output_csv/mapping/p1/train_run1_transfer.txt\"\n",
    "\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in data_train:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test  = r\"/soe/cchakrab/test_output/output_csv/mapping/p1/test_run1_transfer.txt\"\n",
    "with open(path_test, 'w') as f:\n",
    "    for item in data_test:\n",
    "        f.write(\"%s \" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def read_words(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().split()\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"train_run1_transfer.txt\")\n",
    "    test_path = os.path.join(data_path, \"test_run1_transfer.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    #print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, test_data, vocabulary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "997\n",
      "1001 1001 1001 1001 1001 0 3 1001 1001 1001\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, vocabulary, reversed_dictionary = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 1500)          1495500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64, 1500)          18006000  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64, 1500)          18006000  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 1500)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 64, 997)           1496497   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 997)           0         \n",
      "=================================================================\n",
      "Total params: 39,003,997\n",
      "Trainable params: 39,003,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "look_back = 64\n",
    "num_steps = look_back\n",
    "batch_size = 64\n",
    "\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary, skip_step=num_steps) \n",
    "#valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary, skip_step=num_steps)\n",
    "test_data_generator = KerasBatchGenerator(test_data, num_steps, batch_size, vocabulary, skip_step=num_steps)\n",
    "\n",
    "hidden_size = 1500\n",
    "use_dropout=True \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(vocabulary)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy',keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "print(model.summary())\n",
    "monitor = EarlyStopping(monitor='val_categorical_accuracy', min_delta=1e-3, patience=7, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-trained.hdf5', verbose=1)\n",
    "num_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soe/hlitz/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "8376/8376 [==============================] - 3883s 464ms/step - loss: 0.0880 - categorical_accuracy: 0.9918 - precision_1: 0.9922 - recall_1: 0.9913 - val_loss: 0.0621 - val_categorical_accuracy: 0.9927 - val_precision_1: 0.9927 - val_recall_1: 0.9927\n",
      "\n",
      "Epoch 00001: saving model to /soe/cchakrab/test_output/output_csv/mapping/p1/model-trained.hdf5\n",
      "Epoch 2/5000\n",
      "8376/8376 [==============================] - 3874s 463ms/step - loss: 0.0674 - categorical_accuracy: 0.9922 - precision_1: 0.9933 - recall_1: 0.9915 - val_loss: 0.0606 - val_categorical_accuracy: 0.9928 - val_precision_1: 0.9937 - val_recall_1: 0.9920\n",
      "\n",
      "Epoch 00002: saving model to /soe/cchakrab/test_output/output_csv/mapping/p1/model-trained.hdf5\n",
      "Epoch 3/5000\n",
      "5444/8376 [==================>...........] - ETA: 16:02 - loss: 0.0657 - categorical_accuracy: 0.9924 - precision_1: 0.9936 - recall_1: 0.9916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8376/8376 [==============================] - 3903s 466ms/step - loss: 0.0680 - categorical_accuracy: 0.9922 - precision_1: 0.9934 - recall_1: 0.9913 - val_loss: 0.0623 - val_categorical_accuracy: 0.9928 - val_precision_1: 0.9934 - val_recall_1: 0.9924\n",
      "\n",
      "Epoch 00003: saving model to /soe/cchakrab/test_output/output_csv/mapping/p1/model-trained.hdf5\n",
      "Epoch 4/5000\n",
      " 152/8376 [..............................] - ETA: 45:09 - loss: 0.1802 - categorical_accuracy: 0.9788 - precision_1: 0.9821 - recall_1: 0.9764"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-406a5cab6caf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n\u001b[1;32m      2\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                         validation_steps=len(train_data)//(batch_size*num_steps), callbacks=[checkpointer,monitor])\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=test_data_generator.generate(),\n",
    "                        validation_steps=len(train_data)//(batch_size*num_steps), callbacks=[checkpointer,monitor])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained!\n"
     ]
    }
   ],
   "source": [
    "print(\"Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/MSR-Cambridge/Part1/mds_0.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "path  = r\"/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/MSR-Cambridge/Part1/\" \n",
    "\n",
    "names = ['TimeStamp','Host_Name','DiskNumber','Operation_Type','ByteOffset','IOSize','Response_Time','DiskNum']\n",
    "\n",
    "\n",
    "#['TimeStamp','Response','IOType','LUN','ByteOffset','Size']\n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"mds_0.csv\"))\n",
    "print(all_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211034\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(all_files[0],engine='python',skiprows =1,header=None,na_values=['-1'], index_col=False) \n",
    "names = ['TimeStamp','Host_Name','DiskNumber','Operation_Type','ByteOffset','IOSize','Response_Time','DiskNum']\n",
    "\n",
    "df.columns = names\n",
    "print (len(df))\n",
    "# df['ByteOffset_Delta'] = df['ByteOffset'] - df['ByteOffset'].shift(-1)\n",
    "# df = df.drop(df.index[-1])\n",
    "\n",
    "# total_classes = len(Counter(df['ByteOffset_Delta']))\n",
    "# x = Counter(df['ByteOffset_Delta'])\n",
    "# vals = {}\n",
    "# vals =  x.most_common(1000)\n",
    "# bo_list = []\n",
    "# coverage = 0\n",
    "# for key,value in vals:\n",
    "#     bo_list.append(key)\n",
    "#     coverage = coverage + value\n",
    "# print(len(bo_list))\n",
    "# print(\"Percentage Coverage Delta\")\n",
    "# print((coverage/len(df))*100) \n",
    "\n",
    "\n",
    "\n",
    "# total_classes = len(Counter(df['ByteOffset']))\n",
    "# x = Counter(df['ByteOffset'])\n",
    "# vals = {}\n",
    "# vals =  x.most_common(1000)\n",
    "# bo_list = []\n",
    "# coverage = 0\n",
    "# for key,value in vals:\n",
    "#     bo_list.append(key)\n",
    "#     coverage = coverage + value\n",
    "    \n",
    "# print(len(bo_list))\n",
    "# print(\"Percentage Coverage Offset\")\n",
    "# print((coverage/len(df))*100)\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Host_Name</th>\n",
       "      <th>DiskNumber</th>\n",
       "      <th>Operation_Type</th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>IOSize</th>\n",
       "      <th>Response_Time</th>\n",
       "      <th>DiskNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1211029</th>\n",
       "      <td>128172420027898531</td>\n",
       "      <td>mds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3154137088</td>\n",
       "      <td>4096</td>\n",
       "      <td>46799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211030</th>\n",
       "      <td>128172420027902531</td>\n",
       "      <td>mds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3161395200</td>\n",
       "      <td>4096</td>\n",
       "      <td>42800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211031</th>\n",
       "      <td>128172420044631296</td>\n",
       "      <td>mds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>1214406656</td>\n",
       "      <td>4096</td>\n",
       "      <td>345394</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211032</th>\n",
       "      <td>128172420044657931</td>\n",
       "      <td>mds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3233218560</td>\n",
       "      <td>4096</td>\n",
       "      <td>318759</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211033</th>\n",
       "      <td>128172420044911251</td>\n",
       "      <td>mds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write</td>\n",
       "      <td>3221970944</td>\n",
       "      <td>4096</td>\n",
       "      <td>65438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TimeStamp Host_Name  DiskNumber Operation_Type  ByteOffset  \\\n",
       "1211029  128172420027898531       mds         NaN          Write  3154137088   \n",
       "1211030  128172420027902531       mds         NaN          Write  3161395200   \n",
       "1211031  128172420044631296       mds         NaN          Write  1214406656   \n",
       "1211032  128172420044657931       mds         NaN          Write  3233218560   \n",
       "1211033  128172420044911251       mds         NaN          Write  3221970944   \n",
       "\n",
       "         IOSize  Response_Time  DiskNum  \n",
       "1211029    4096          46799        0  \n",
       "1211030    4096          42800        0  \n",
       "1211031    4096         345394        0  \n",
       "1211032    4096         318759        0  \n",
       "1211033    4096          65438        0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Operation_type, Host_name and remove Disk_Num\n",
    "# Make Class of Byte Offset delta \n",
    "# Normalize the columns (Except ByteOffset Delta)\n",
    "# Make the prediction model with 1000 classes\n",
    "\n",
    "     \n",
    "\n",
    "address_map = dict(Counter(df['ByteOffset']))\n",
    "sorted_address_map = sorted(dict(address_map).items(), key=operator.itemgetter(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997\n",
      "279551\n"
     ]
    }
   ],
   "source": [
    "address_remap = {}\n",
    "\n",
    "count = 0\n",
    "while(count<len(sorted_address_map)):\n",
    "    if(count < 995):\n",
    "        address_remap[count] = sorted_address_map[len(sorted_address_map)-count - 1][0]\n",
    "    else:\n",
    "        address_remap[995] = sorted_address_map[len(sorted_address_map)-count - 1][0]\n",
    "    count = count + 1\n",
    "\n",
    "rev_address_remap = {y:x for x,y in address_remap.items()}\n",
    "\n",
    "count = 0\n",
    "label_list = []\n",
    "rogue_delta = []\n",
    "label =0.0\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset'].iloc[count]\n",
    "    if x in rev_address_remap:\n",
    "        label = rev_address_remap[x]\n",
    "        label_list.append(label)\n",
    "    else:\n",
    "        label_list.append(1001)\n",
    "        rogue_delta.append(x)\n",
    "    count = count+1\n",
    "\n",
    "df['ByteOffset_class']  = label_list\n",
    "print(len(Counter(df['ByteOffset_class'])))\n",
    "print(len(rogue_delta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_test = df['ByteOffset_class'].values.tolist()\n",
    "\n",
    "path_train  = r\"/soe/cchakrab/test_output/output_csv/mapping/p1/mapping_run2_transfer.txt\"\n",
    "\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in mapping_test:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_test = df['ByteOffset_class'].values.tolist()\n",
    "\n",
    "path_train  = r\"/soe/cchakrab/test_output/output_csv/mapping/p1/mapping_run2_transfer.txt\"\n",
    "\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in mapping_test:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "data_path = r\"/soe/cchakrab/test_output/output_csv/mapping/p1/\"\n",
    "\n",
    "def read_words(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().split()\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"mapping_run2_transfer.txt\")\n",
    "    #test_path = os.path.join(data_path, \"test_run2_transfer.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    #test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    #print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, vocabulary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 5, 429]\n",
      "997\n",
      "1001 0 1001 4 428 427 624 1 623 1001\n"
     ]
    }
   ],
   "source": [
    "mapping_data, vocabulary, reversed_dictionary = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data_generator = KerasBatchGenerator(mapping_data, num_steps, batch_size, vocabulary, skip_step=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  7.149606704711914 Accuracy:  0.23075346648693085\n",
      "--- 2591.0418541431427 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "samples = (len(mapping_data)/batch_size)\n",
    "score = model.evaluate_generator(mapping_data_generator.generate(), samples)\n",
    "print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done...!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
