{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Chandranil Chakraborttii\n",
    "# Project: Learning I/O Access Patterns to Improve Prefetching in SSDs\n",
    "# Paper Link :https://www.researchgate.net/profile/Chandranil_Chakraborttii/publication/344379801_Learning_IO_Access_Patterns_to_Improve_Prefetching_in_SSDs/links/5f6e28fba6fdcc00863adb13/Learning-I-O-Access-Patterns-to-Improve-Prefetching-in-SSDs.pdf\n",
    "# Loading libraries\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (trace 1)\n",
    "# Map, order by frequency \n",
    "# Train model \n",
    "# Load another data (trace 2)\n",
    "# Map, order by frequency - Same as trace 1\n",
    "# Load model\n",
    "# Use the model to predict on the new trace\n",
    "# Note and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/Buildserver/data-output-buildserver-2_total.csv']\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path  = r\"/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/Buildserver/\" \n",
    "\n",
    "names = ['Operation','TimeStamp','Process_Name','ThreadID','IrpPtr','ByteOffset','IOSize','ThreadID','ElapsedTime','DiskNum','IrpFlags','DiskSvcTime','I/O Pri','VolSnap','FileObject','FileName','IO_Pri']\n",
    "\n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"data-output-buildserver-2_total.csv\"))\n",
    "print(all_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600438\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(all_files[0],engine='python',skiprows =1,header=None,na_values=['-1'], index_col=False) \n",
    "names = ['Operation','TimeStamp','Process_Name','ThreadID','IrpPtr','ByteOffset','IOSize','ThreadID','ElapsedTime','DiskNum','IrpFlags','DiskSvcTime','I/O Pri','VolSnap','FileObject','FileName','IO_Pri']\n",
    "\n",
    "\n",
    "df.columns = names\n",
    "#Sorting df by TimeStamp\n",
    "\n",
    "df = df.sort_values(by=['TimeStamp'])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "addresses = df['ByteOffset'].tolist()\n",
    "addresses_dec = []\n",
    "count = 0\n",
    "\n",
    "while (count < len(addresses)):\n",
    "    if \"Offset\" in addresses[count]:\n",
    "        count = count +1\n",
    "        continue\n",
    "    dec = addresses[count]\n",
    "    tmp = int(dec, 16)\n",
    "    addresses_dec.append(tmp)\n",
    "    count = count +1\n",
    "    \n",
    "df_a = pd.DataFrame(columns = ['ByteOffset', 'ByteOffset_Delta']) \n",
    "\n",
    "df_a['ByteOffset'] = addresses_dec\n",
    "\n",
    "df = df_a\n",
    "\n",
    "df['ByteOffset_Delta'] = df['ByteOffset'] - df['ByteOffset'].shift(-1)\n",
    "df = df.drop(df.index[-1])\n",
    "\n",
    "# total_classes = len(Counter(df['ByteOffset_Delta']))\n",
    "# x = Counter(df['ByteOffset_Delta'])\n",
    "# vals = {}\n",
    "# vals =  x.most_common(1000)\n",
    "# bo_list = []\n",
    "# coverage = 0\n",
    "# for key,value in vals:\n",
    "#     bo_list.append(key)\n",
    "#     coverage = coverage + value\n",
    "# print(len(bo_list))\n",
    "# print(\"Percentage Coverage Delta\")\n",
    "# print((coverage/len(df))*100) \n",
    "\n",
    "\n",
    "# total_classes = len(Counter(df['ByteOffset']))\n",
    "# x = Counter(df['ByteOffset'])\n",
    "# vals = {}\n",
    "# vals =  x.most_common(1000)\n",
    "# bo_list = []\n",
    "# coverage = 0\n",
    "# for key,value in vals:\n",
    "#     bo_list.append(key)\n",
    "#     coverage = coverage + value\n",
    "    \n",
    "# print(len(bo_list))\n",
    "# print(\"Percentage Coverage Offset\")\n",
    "# print((coverage/len(df))*100)\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Operation_type, Host_name and remove Disk_Num\n",
    "# Make Class of Byte Offset delta \n",
    "# Normalize the columns (Except ByteOffset Delta)\n",
    "# Make the prediction model with 1000 classes\n",
    "import operator\n",
    "     \n",
    "df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "address_map = dict(Counter(df['ByteOffset_Delta']))\n",
    "sorted_address_map = sorted(dict(address_map).items(), key=operator.itemgetter(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_remap = {}\n",
    "\n",
    "count = 0\n",
    "while(count<len(sorted_address_map)):\n",
    "    if(count < 1000):\n",
    "        address_remap[count] = sorted_address_map[len(sorted_address_map)-count - 1][0]\n",
    "    else:\n",
    "        address_remap[1001] = sorted_address_map[len(sorted_address_map)-count - 1][0]\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_address_remap = {y:x for x,y in address_remap.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "1474019\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "label_list = []\n",
    "rogue_delta = []\n",
    "label =0.0\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset_Delta'].iloc[count]\n",
    "    if x in rev_address_remap:\n",
    "        label = rev_address_remap[x]\n",
    "        label_list.append(label)\n",
    "    else:\n",
    "        label_list.append(1001)\n",
    "        rogue_delta.append(x)\n",
    "    count = count+1\n",
    "\n",
    "df['ByteOffset_Delta_class']  = label_list\n",
    "print(len(Counter(df['ByteOffset_Delta_class'])))\n",
    "print(len(rogue_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2435"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label_list)[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ByteOffset</th>\n",
       "      <th>ByteOffset_Delta</th>\n",
       "      <th>ByteOffset_Delta_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>251824893952</td>\n",
       "      <td>-6.752605e+10</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319350947840</td>\n",
       "      <td>2.375499e+11</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81801076736</td>\n",
       "      <td>-1.043041e+11</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186105143296</td>\n",
       "      <td>-5.477419e+10</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240879329280</td>\n",
       "      <td>1.861192e+11</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ByteOffset  ByteOffset_Delta  ByteOffset_Delta_class\n",
       "0  251824893952     -6.752605e+10                    1001\n",
       "1  319350947840      2.375499e+11                    1001\n",
       "2   81801076736     -1.043041e+11                    1001\n",
       "3  186105143296     -5.477419e+10                    1001\n",
       "4  240879329280      1.861192e+11                    1001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200321\n",
      "400107\n"
     ]
    }
   ],
   "source": [
    "# Split to train, validate and test\n",
    "\n",
    "# Finding the value 75th percentile of TimeStamp\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "training_pt_1 = math.floor((len(df)*0.75)) \n",
    "\n",
    "\n",
    "data_train = df['ByteOffset_Delta_class'][:training_pt_1]\n",
    "data_test = df['ByteOffset_Delta_class'][training_pt_1+1:]\n",
    "\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "len(Counter(df['ByteOffset_Delta_class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = r'/soe/cchakrab/test_output/output_csv/BuildServer/p5'\n",
    "path_train  = r\"/soe/cchakrab/test_output/output_csv/BuildServer/p5/train_run1_transfer.txt\"\n",
    "\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in data_train:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test  = r\"/soe/cchakrab/test_output/output_csv/BuildServer/p5/test_run1_transfer.txt\"\n",
    "with open(path_test, 'w') as f:\n",
    "    for item in data_test:\n",
    "        f.write(\"%s \" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def read_words(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().split()\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"train_run1_transfer.txt\")\n",
    "    test_path = os.path.join(data_path, \"test_run1_transfer.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    #print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, test_data, vocabulary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n",
      "855\n",
      "1001 1001 1001 1001 1001 1001 1001 1001 1001 1001\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, vocabulary, reversed_dictionary = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 1500)          1282500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64, 1500)          18006000  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64, 1500)          18006000  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 1500)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 64, 855)           1283355   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64, 855)           0         \n",
      "=================================================================\n",
      "Total params: 38,577,855\n",
      "Trainable params: 38,577,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "look_back = 64\n",
    "num_steps = look_back\n",
    "batch_size = 64\n",
    "\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary, skip_step=num_steps) \n",
    "#valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary, skip_step=num_steps)\n",
    "test_data_generator = KerasBatchGenerator(test_data, num_steps, batch_size, vocabulary, skip_step=num_steps)\n",
    "\n",
    "hidden_size = 1500\n",
    "use_dropout=True \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(vocabulary)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy',keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "print(model.summary())\n",
    "monitor = EarlyStopping(monitor='val_categorical_accuracy', min_delta=1e-3, patience=7, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-trained.hdf5', verbose=1)\n",
    "num_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soe/hlitz/miniconda3/envs/py3_tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "293/293 [==============================] - 138s 470ms/step - loss: 0.7761 - categorical_accuracy: 0.9178 - precision_1: 0.9225 - recall_1: 0.9114 - val_loss: 0.4126 - val_categorical_accuracy: 0.9285 - val_precision_1: 0.9313 - val_recall_1: 0.9222\n",
      "\n",
      "Epoch 00001: saving model to /soe/cchakrab/test_output/output_csv/BuildServer/p5/model-trained.hdf5\n",
      "Epoch 2/5000\n",
      "293/293 [==============================] - 137s 468ms/step - loss: 0.6830 - categorical_accuracy: 0.9206 - precision_1: 0.9216 - recall_1: 0.9185 - val_loss: 0.3985 - val_categorical_accuracy: 0.9274 - val_precision_1: 0.9278 - val_recall_1: 0.9272\n",
      "\n",
      "Epoch 00002: saving model to /soe/cchakrab/test_output/output_csv/BuildServer/p5/model-trained.hdf5\n",
      "Epoch 3/5000\n",
      "293/293 [==============================] - 138s 470ms/step - loss: 0.6121 - categorical_accuracy: 0.9211 - precision_1: 0.9237 - recall_1: 0.9182 - val_loss: 0.3214 - val_categorical_accuracy: 0.9278 - val_precision_1: 0.9341 - val_recall_1: 0.9213\n",
      "\n",
      "Epoch 00003: saving model to /soe/cchakrab/test_output/output_csv/BuildServer/p5/model-trained.hdf5\n",
      "Epoch 4/5000\n",
      "293/293 [==============================] - 137s 468ms/step - loss: 0.5328 - categorical_accuracy: 0.9201 - precision_1: 0.9288 - recall_1: 0.9143 - val_loss: 0.2371 - val_categorical_accuracy: 0.9289 - val_precision_1: 0.9333 - val_recall_1: 0.9268\n",
      "\n",
      "Epoch 00004: saving model to /soe/cchakrab/test_output/output_csv/BuildServer/p5/model-trained.hdf5\n",
      "Epoch 5/5000\n",
      "293/293 [==============================] - 137s 468ms/step - loss: 0.5028 - categorical_accuracy: 0.9209 - precision_1: 0.9303 - recall_1: 0.9156 - val_loss: 0.4214 - val_categorical_accuracy: 0.9280 - val_precision_1: 0.9344 - val_recall_1: 0.9254\n",
      "\n",
      "Epoch 00005: saving model to /soe/cchakrab/test_output/output_csv/BuildServer/p5/model-trained.hdf5\n",
      "Epoch 6/5000\n",
      "293/293 [==============================] - 137s 469ms/step - loss: 0.4981 - categorical_accuracy: 0.9205 - precision_1: 0.9309 - recall_1: 0.9151 - val_loss: 0.2410 - val_categorical_accuracy: 0.9288 - val_precision_1: 0.9360 - val_recall_1: 0.9255\n",
      "\n",
      "Epoch 00006: saving model to /soe/cchakrab/test_output/output_csv/BuildServer/p5/model-trained.hdf5\n",
      "Epoch 7/5000\n",
      "293/293 [==============================] - 137s 469ms/step - loss: 0.4922 - categorical_accuracy: 0.9205 - precision_1: 0.9312 - recall_1: 0.9150 - val_loss: 0.6901 - val_categorical_accuracy: 0.9285 - val_precision_1: 0.9354 - val_recall_1: 0.9253\n",
      "\n",
      "Epoch 00007: saving model to /soe/cchakrab/test_output/output_csv/BuildServer/p5/model-trained.hdf5\n",
      "Epoch 8/5000\n",
      "293/293 [==============================] - 139s 473ms/step - loss: 0.4888 - categorical_accuracy: 0.9204 - precision_1: 0.9303 - recall_1: 0.9147 - val_loss: 0.5476 - val_categorical_accuracy: 0.9282 - val_precision_1: 0.9353 - val_recall_1: 0.9249\n",
      "\n",
      "Epoch 00008: saving model to /soe/cchakrab/test_output/output_csv/BuildServer/p5/model-trained.hdf5\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff12e222f28>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=test_data_generator.generate(),\n",
    "                        validation_steps=len(train_data)//(batch_size*num_steps), callbacks=[checkpointer,monitor])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained!\n"
     ]
    }
   ],
   "source": [
    "print(\"Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.sequential.Sequential"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/2016022315-LUN2.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import metrics\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import insert\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path  = r\"/soe/hlitz/maxwell/notebooks/Prefetching_SSD/Data/output/\" \n",
    "\n",
    "names = ['TimeStamp','ByteOffset']\n",
    "\n",
    "\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"2016022315-LUN2.csv\"))\n",
    "print(all_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5226120\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(all_files[0],engine='python',skiprows =1,header=None,na_values=['-1'], index_col=False) \n",
    "names = ['TimeStamp','ByteOffset']   \n",
    "df.columns = names\n",
    "print (len(df))\n",
    "df.TimeStamp = df.TimeStamp.astype(int)\n",
    "df.ByteOffset = df.ByteOffset.astype(int)\n",
    "df['ByteOffset_Delta'] = df['ByteOffset'] - df['ByteOffset'].shift(-1)\n",
    "df = df.drop(df.index[-1])\n",
    "\n",
    "# total_classes = len(Counter(df['ByteOffset_Delta']))\n",
    "# x = Counter(df['ByteOffset_Delta'])\n",
    "# vals = {}\n",
    "# vals =  x.most_common(1000)\n",
    "# bo_list = []\n",
    "# coverage = 0\n",
    "# for key,value in vals:\n",
    "#     bo_list.append(key)\n",
    "#     coverage = coverage + value\n",
    "# print(len(bo_list))\n",
    "# print(\"Percentage Coverage\")\n",
    "# print((coverage/len(df))*100) \n",
    "# total_classes = len(Counter(df['ByteOffset']))\n",
    "# x = Counter(df['ByteOffset'])\n",
    "# vals = {}\n",
    "# vals =  x.most_common(1000)\n",
    "# bo_list = []\n",
    "# coverage = 0\n",
    "# for key,value in vals:\n",
    "#     bo_list.append(key)\n",
    "#     coverage = coverage + value\n",
    "    \n",
    "# print(len(bo_list))\n",
    "# print(\"Percentage Coverage Offset\")\n",
    "# print((coverage/len(df))*100)\n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Operation_type, Host_name and remove Disk_Num\n",
    "# Make Class of Byte Offset delta \n",
    "# Normalize the columns (Except ByteOffset Delta)\n",
    "# Make the prediction model with 1000 classes\n",
    "\n",
    "     \n",
    "df['ByteOffset_Delta'] = df['ByteOffset_Delta'].fillna(0)\n",
    "address_map = dict(Counter(df['ByteOffset_Delta']))\n",
    "sorted_address_map = sorted(dict(address_map).items(), key=operator.itemgetter(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855\n",
      "2163400\n"
     ]
    }
   ],
   "source": [
    "address_remap = {}\n",
    "\n",
    "count = 0\n",
    "while(count<len(sorted_address_map)):\n",
    "    if(count < 853):\n",
    "        address_remap[count] = sorted_address_map[len(sorted_address_map)-count - 1][0]\n",
    "    else:\n",
    "        address_remap[853] = sorted_address_map[len(sorted_address_map)-count - 1][0]\n",
    "    count = count + 1\n",
    "\n",
    "rev_address_remap = {y:x for x,y in address_remap.items()}\n",
    "\n",
    "count = 0\n",
    "label_list = []\n",
    "rogue_delta = []\n",
    "label =0.0\n",
    "while (count < len(df)):\n",
    "    x = df['ByteOffset_Delta'].iloc[count]\n",
    "    if x in rev_address_remap:\n",
    "        label = rev_address_remap[x]\n",
    "        label_list.append(label)\n",
    "    else:\n",
    "        label_list.append(1001)\n",
    "        rogue_delta.append(x)\n",
    "    count = count+1\n",
    "\n",
    "df['ByteOffset_Delta_class']  = label_list\n",
    "print(len(Counter(df['ByteOffset_Delta_class'])))\n",
    "print(len(rogue_delta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_test = df['ByteOffset_Delta_class'].values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train  = r\"/soe/cchakrab/test_output/output_csv/BuildServer/p5/train_run2_transfer.txt\"\n",
    "\n",
    "with open(path_train, 'w') as f:\n",
    "    for item in mapping_test:\n",
    "        f.write(\"%s \" % item)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def read_words(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read().split()\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"train_run2_transfer.txt\")\n",
    "    #test_path = os.path.join(data_path, \"test_run2_transfer.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    #test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    #print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, vocabulary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[854, 155, 52, 255, 4]\n",
      "855\n",
      "853 154 51 254 3 384 296 194 245 266\n"
     ]
    }
   ],
   "source": [
    "mapping_data, vocabulary, reversed_dictionary = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5226119"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data_generator = KerasBatchGenerator(mapping_data, num_steps, batch_size, vocabulary, skip_step=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5226119"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "samples = (len(mapping_data)/batch_size)\n",
    "score = model.evaluate_generator(mapping_data_generator.generate(), samples)\n",
    "print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done...!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
